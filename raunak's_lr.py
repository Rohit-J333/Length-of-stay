# -*- coding: utf-8 -*-
"""raunak's lr.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LESxtETLwMbonp7dpGZOhdB5hHHxrHpl

# Logistic Regression

### Imports
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.optim.lr_scheduler as sch
from torch.utils.data import Dataset,DataLoader, random_split
import math

"""### Data Preprocessing"""

def transform_data(df):

  df.drop(['CCS Diagnosis Description', 'CCS Procedure Description', 'APR DRG Description', 'APR MDC Description',
                 'APR Severity of Illness Description','Payment Typology 2','Payment Typology 3','Race','Gender','Birth Weight'], axis=1, inplace=True)
  ga = math.pow(10,-15)
  df.loc[:, 'Length of Stay'] = df['Length of Stay'].replace('120 +',120)
  df.loc[:, 'Length of Stay'] = df['Length of Stay'].astype(np.int)
  x = np.where(df['Length of Stay'] >= 8,7,df['Length of Stay']-1)
  df.drop(['Length of Stay'], axis=1, inplace=True)
  df['Length of Stay'] = x
  q = df['Total Costs'].values
  q = q/1000000
  df['Total Costs'] = -np.log(np.clip(q,ga,1-ga))
  df = df.sample(frac=1)

  names = df.columns
  n = len(names)
  i = 0
  names1 = []
  def remove(string):
    return string.replace(" ", "")
  while(i < n):
      s = remove(names[i])
      if(s == "ZipCode-3digits"):
          s = "ZipCodedigits"
      names1.append(s)
      i = i+1

  df.columns = names1
  r = df.shape[0]
  r = int(r/10)
  arr = []
  cols_to_oh_encode = ["HealthServiceArea", "HospitalCounty", "OperatingCertificateNumber", "FacilityId", "FacilityName", "AgeGroup", "ZipCodedigits", "Ethnicity", "TypeofAdmission", "PatientDisposition", "CCSDiagnosisCode", "CCSProcedureCode", "APRDRGCode", "APRMDCCode", "APRSeverityofIllnessCode", "APRRiskofMortality", "APRMedicalSurgicalDescription", "PaymentTypology1", "EmergencyDepartmentIndicator"]

  index = 0
  for col in cols_to_oh_encode:
    uniq_vals = df[col].unique()
    mapping = {k: v for v, k in enumerate(uniq_vals)}
    df[col] = df[col].map(mapping)
    arr.append(len(uniq_vals))
  return df[cols_to_oh_encode].values, df["LengthofStay"].values, df["TotalCosts"].values, arr, cols_to_oh_encode,

"""### Defining the dataset class

The file path of the dataset in text format is to be specified in the below cell
"""

DATA_FILE_PATH = "/content/Hospital_Inpatient_Discharges__SPARCS_De-Identified___2019.csv"
class HospitalDataset(Dataset):
    def __init__(self):
        df = pd.read_csv(DATA_FILE_PATH, engine = 'python') # The path here has to be replaced with the actual path
        self.X, self.Y, self.total_costs, self.d, self.cols_to_oh_encode = transform_data(df)


    def __len__(self):
        return self.X.shape[0]
    def oh_encode(self, idx):
        x = self.X[idx]
        tot_cost = self.total_costs[idx]
        c = x.shape[0]
        X1 = np.zeros(1610)
        X4 = np.ones(1)
        index = 0
        for i, j in enumerate(self.d):
            X1[index + x[i]] = tot_cost
            index += j

        X = np.concatenate((X1,X4),axis=0)
        return X
    def __getitem__(self,idx):
        Y = self.Y[idx]
        X = self.oh_encode(idx)

        return torch.tensor(X, dtype=torch.float32), torch.tensor(Y)

"""### Division into Train, Validation, Test Dataset"""

dataset = HospitalDataset()
r = len(dataset)
train_size, val_size, test_size = int(r*0.9), int(r*0.05), r - int(r*0.05) - int(r*0.9)
train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42))
print(f"{train_size} training samples, {val_size} validation samples, {test_size} test samples")
train_loader = DataLoader(dataset=train_dataset, batch_size=70, shuffle=False)
val_loader = DataLoader(dataset=val_dataset, batch_size=70, shuffle=False)
test_loader = DataLoader(dataset=test_dataset, batch_size=70, shuffle=False)
from google.colab import drive
drive.mount('/content/drive')

"""### Defining Logistic regression"""

class LR(nn.Module):
    def __init__(self):
        super(LR, self).__init__()
        self.fc1 = nn.Linear(1611,1024)

    def forward(self, x):
        x = self.fc1(x)
        x = F.softmax(x, dim=1)
        return x

device = 'cuda' if torch.cuda.is_available() else 'cpu'

"""### Running Logistic regresssion"""

model  = LR().to(device)
crit = nn.CrossEntropyLoss()
crit = crit.to(device)
opti = optim.Adam(model.parameters(),lr=0.001,weight_decay=0.0001)
lr = sch.StepLR(opti,step_size=5,gamma=0.1)
epochs = 10

i = 1
while(i <= epochs):
    print(f"At epoch {i} of {epochs} epochs")
    model.train()
    epoch_loss = 0
    for X, Y in train_loader:
        X = X.to(device)
        Y = Y.to(device)


        output = model(X)

        loss = crit(output,Y.long())
        opti.zero_grad()
        loss.backward()
        epoch_loss += loss.item()
        opti.step()


    ncorr = 0
    nsam = 0
    model.eval()

    with torch.no_grad():
        for X, Y in val_loader:
            X = X.to(device)
            Y = Y.to(device)
            output = model(X)
            _, pred = output.max(1)
            nsam = nsam + Y.size(0)
            ncorr = ncorr + pred.eq(Y).sum().item()

    print(f"At the end of epoch {i}, {ncorr} correct predictions, accuracy = {ncorr/nsam}, training loss = {epoch_loss}")


    i = i+1

ncorr = 0
nsam = 0
model.eval()

with torch.no_grad():
    for batch_idx,sample in test_loader:
        X = X.to(device)
        Y = Y.to(device)
        output = model(X)
        _, pred = output.max(1)
        nsam = nsam + Y.size(0)
        ncorr = ncorr + pred.eq(Y).sum().item()
print(ncorr,nsam)
accr = ncorr/nsam
print(accr)

"""### Optional: Install shap"""

pip install shap
import shap

"""### Plotting shap values: Not used

"""

explainer = shap.DeepExplainer(model, next(iter(test_loader))[0].to(device),)
shap_values = explainer.shap_values(next(iter(test_loader))[0].to(device),)

d = dataset.d
def transform_numpy_matrix(m):
  index = 0
  x = np.zeros((m.shape[0], len(d)))
  for k, i in enumerate(d):
    x[:, k] = np.sum(m[:, index:index+i], axis=1)
    index += i
  return x
shap_values = [transform_numpy_matrix(m) for m in shap_values]

shap.summary_plot(shap_values,feature_names = dataset.cols_to_oh_encode, plot_type="bar")

"""# Linear Regression

### Imports
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

from sklearn.model_selection import KFold
import torch.optim.lr_scheduler as sch
from torch.utils.data import Dataset, DataLoader, random_split
import math

"""### Data preprocessing"""

def clean_data(df):
    df = df.dropna(subset=['Health Service Area','Hospital County','Operating Certificate Number',
                      'Facility Id','Zip Code - 3 digits',
                      'APR Severity of Illness Description','APR Risk of Mortality'])
    df.drop(df[df["Type of Admission"]==3].index, axis = 0, inplace=True)
    df.drop(['CCS Diagnosis Description', 'CCS Procedure Description', 'APR DRG Description', 'APR MDC Description', 'Total Costs', 'Total Charges', 'Discharge Year','Abortion Edit Indicator',
                  'APR Severity of Illness Description','Payment Typology 2','Payment Typology 3','Race','Gender','Birth Weight'], axis=1, inplace=True)

    df.loc[:, 'Length of Stay'] = df['Length of Stay'].replace('120 +',120)
    df.loc[:, 'Length of Stay'] = df['Length of Stay'].astype(np.int)

    df = df.sample(frac=1)

    names = df.columns
    n = len(names)
    i = 0
    names1 = []
    def remove(string):
      return string.replace(" ", "")
    while(i < n):
        s = remove(names[i])
        if(s == "ZipCode-3digits"):
            s = "ZipCodedigits"
        names1.append(s)
        i = i+1

    df.columns = names1

    return df

"""### Numerically encoding the data"""

def transform_data(df):

  cols_to_oh_encode = ["HealthServiceArea", "HospitalCounty", "OperatingCertificateNumber", "FacilityId", "FacilityName", "AgeGroup", "ZipCodedigits", "Ethnicity", "TypeofAdmission", "PatientDisposition", "CCSDiagnosisCode", "CCSProcedureCode", "APRDRGCode", "APRMDCCode", "APRSeverityofIllnessCode", "APRRiskofMortality", "APRMedicalSurgicalDescription", "PaymentTypology1", "EmergencyDepartmentIndicator"]

  index = 0
  arr = []
  for col in cols_to_oh_encode:
    uniq_vals = df[col].unique()
    mapping = {k: v for v, k in enumerate(uniq_vals)}
    df[col] = df[col].map(mapping)
    arr.append(len(uniq_vals))
  return df[cols_to_oh_encode].values, df["LengthofStay"].values, arr, cols_to_oh_encode,

"""### Defining the hospital dataset"""

class HospitalDataset(Dataset):
    def __init__(self, df, ):
        df = clean_data(df)
        self.X, self.Y, self.d, self.cols_to_oh_encode = transform_data(df)

    def __len__(self):
        return self.X.shape[0]

    def oh_encode(self, idx):
        x = self.X[idx]

        X = np.zeros(1610)

        index = 0
        for i, j in enumerate(self.d):
            X[index + x[i]] = 1
            index += j


        return X
    def __getitem__(self, idx):
        Y = self.Y[idx]
        X = self.oh_encode(idx)

        return torch.tensor(X, dtype=torch.float32), torch.tensor(Y, dtype=torch.float32)

"""### Using k fold cross validation"""

k_f = KFold(n_splits=10,)

"""### Putting dataset into memory

The path of the dataset file is in the cell below
"""

DATA_FILE_PATH = '/content/Hospital_Inpatient_Discharges__SPARCS_De-Identified___2019.csv'
df = pd.read_csv(DATA_FILE_PATH)
dataset = HospitalDataset(df, )
r = len(dataset)
train_val_size, test_size = int(r*0.90), r - int(r*0.10)
train_val_dataset, test_dataset = random_split(dataset, [train_val_size, test_size], generator=torch.Generator().manual_seed(42))

print(f"{train_val_size} training and validation samples, {test_size} test samples")
device = 'cuda' if torch.cuda.is_available() else 'cpu'

"""### Function for training and evaluating"""

def train_and_evaluate(train_loader, val_loader, n_epochs=5):
    model  = nn.Linear(1610, 1).to(device)
    crit = nn.MSELoss()
    crit = crit.to(device)
    opti = optim.Adam(model.parameters(),lr=0.001,weight_decay=0.0001)
    lr = sch.StepLR(opti,step_size=5,gamma=0.1)
    val_size = 0

    i = 1

    model.eval()
    x_squared_sum = 0
    x_sum = 0
    o_x_sum = 0
    x_mean = 0
    o_squared_sum = 0
    for x, y in val_loader:
        val_size += x.shape[0]
        with torch.no_grad():
            y = y.to(device)
            x = x.to(device)
            o = model(x)
            o = o.squeeze(-1)
            x_squared_sum += torch.sum(y**2)
            x_sum += torch.sum(y)
            o_x_sum += torch.sum (torch.dot(o, y))
            o_squared_sum += torch.sum(o**2)
            target_sum = torch.sum(y)
    x_mean = x_sum / val_size
    r2_score_numerator = x_squared_sum + o_squared_sum - 2 * o_x_sum
    r2_score_denominator = x_squared_sum + val_size * (x_mean**2) - 2 * (x_sum) * x_mean


    print(f"Before the start of epoch 1, r2 score = {1 - r2_score_numerator/r2_score_denominator}")

    while(i <= n_epochs):
        print(f"At epoch {i} of {n_epochs} epochs")
        model.train()
        epoch_loss = 0
        for X, Y in train_loader:
            X = X.to(device)
            Y = Y.to(device)


            output = model(X)

            loss = crit(torch.squeeze(output),Y.float())
            opti.zero_grad()
            loss.backward()
            epoch_loss += loss.item()
            opti.step()

        model.eval()
        x_squared_sum = 0
        x_sum = 0
        o_x_sum = 0
        x_mean = 0
        o_squared_sum = 0
        for x, y in val_loader:

          with torch.no_grad():
            y = y.to(device)
            x = x.to(device)
            o = model(x)
            o = o.squeeze(-1)
            x_squared_sum += torch.sum(y**2)
            x_sum += torch.sum(y)
            o_x_sum += torch.sum (torch.dot(o, y))
            o_squared_sum += torch.sum(o**2)
            target_sum = torch.sum(y)
          x_mean = x_sum / val_size
          r2_score_numerator = x_squared_sum + o_squared_sum - 2 * o_x_sum
          r2_score_denominator = x_squared_sum + val_size * (x_mean**2) - 2 * (x_sum) * x_mean


        print(f"At the end of epoch {i}, r2 score = {1 - r2_score_numerator/r2_score_denominator}, training loss = {epoch_loss}")

        i = i+1

len(train_val_dataset)

"""### Doing cross validation"""

for fold, (train_ids, val_ids) in enumerate(k_f.split(train_val_dataset)):

    # Print
    print(f'FOLD {fold}')
    print('--------------------------------')

    # Sample elements randomly from a given list of ids, no replacement.
    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)
    val_subsampler = torch.utils.data.SubsetRandomSampler(val_ids)
    train_loader = DataLoader(
                      dataset,
                      batch_size=70, sampler=train_subsampler)
    val_loader = DataLoader(
                      dataset,
                      batch_size=70, sampler=val_subsampler)

    train_and_evaluate(train_loader, val_loader, n_epochs=5)





train_loader = DataLoader(dataset=train_val_dataset, batch_size=70, shuffle=False)

test_loader = DataLoader(dataset=test_dataset, batch_size=70, shuffle=False)
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model  = nn.Linear(1610, 1).to(device)
crit = nn.MSELoss()
crit = crit.to(device)
opti = optim.Adam(model.parameters(),lr=0.001,weight_decay=0.0001)
lr = sch.StepLR(opti,step_size=5,gamma=0.1)
epochs = 10

i = 0
while(i <= epochs):
    print(f"At epoch {i} of {epochs} epochs")
    model.train()
    epoch_loss = 0
    for X, Y in train_loader:
        X = X.to(device)
        Y = Y.to(device)


        output = model(X)

        loss = crit(torch.squeeze(output),Y.float())
        opti.zero_grad()
        loss.backward()
        epoch_loss += loss.item()
        opti.step()
    i += 1

model.eval()
x_squared_sum = 0
x_sum = 0
o_x_sum = 0
x_mean = 0
o_squared_sum = 0
val_size = 0
for x, y in test_loader:
    val_size += x.shape[0]
    with torch.no_grad():
        y = y.to(device)
        x = x.to(device)
        o = model(x)
        o = o.squeeze(-1)
        x_squared_sum += torch.sum(y**2)
        x_sum += torch.sum(y)
        o_x_sum += torch.sum (torch.dot(o, y))
        o_squared_sum += torch.sum(o**2)
        target_sum = torch.sum(y)
x_mean = x_sum / val_size
r2_score_numerator = x_squared_sum + o_squared_sum - 2 * o_x_sum
r2_score_denominator = x_squared_sum + val_size * (x_mean**2) - 2 * (x_sum) * x_mean


print(f"Test r2 score = {1 - r2_score_numerator/r2_score_denominator}, training loss = {epoch_loss}")

plt.scatter(output, ans)
m, b = np.polyfit(output,ans,1)
plt.plot(output,m*output+b,"k")
plt.plot(output,output,"r")
plt.plot(output,output-5,"m")
plt.plot(output,output+5,"m")
plt.legend(['Best Fit Line','Exact Line'])
plt.title("Scatterplot for length of stay")

plt.clf()

"""# Random Forest for regression"""

# @title Import
!pip install shap
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

from torch.utils.data import Dataset,DataLoader, random_split
import math
from sklearn.ensemble import RandomForestRegressor

# @title Data Preprocessing and Transforming
def transform_data(df):
  df = df.dropna(subset=['Health Service Area','Hospital County','Operating Certificate Number',
                      'Facility Id','Zip Code - 3 digits',
                      'APR Severity of Illness Description','APR Risk of Mortality'])
  df.drop(['CCS Diagnosis Description', 'CCS Procedure Description','Discharge Year','Abortion Edit Indicator', 'APR DRG Description', 'APR MDC Description', 'Total Costs', 'Total Charges',
                 'APR Severity of Illness Description','Payment Typology 2','Payment Typology 3','Race','Gender','Birth Weight'], axis=1, inplace=True)

  ga = math.pow(10,-15)
  df.loc[:, 'Length of Stay'] = df['Length of Stay'].replace('120 +',120)
  df.loc[:, 'Length of Stay'] = df['Length of Stay'].astype(np.int)

  df = df.sample(frac=1)

  names = df.columns
  n = len(names)
  i = 0
  names1 = []
  def remove(string):
    return string.replace(" ", "")
  while(i < n):
      s = remove(names[i])
      if(s == "ZipCode-3digits"):
          s = "ZipCodedigits"
      names1.append(s)
      i = i+1

  df.columns = names1

  obj_columns = df.select_dtypes(include=['object']).columns

  for col in obj_columns:
      try:
        uniq_values = df[col].unique()
      except:
        print("the column giving error is, "+ col)
        break
      mapping = {k: v for v, k in enumerate(uniq_values)}
      df[col] = df[col].map(mapping)
      try:
          geeky_file = open('encodings.txt', 'a+')
          geeky_file.write("\n\n"+col+"\n"+str(mapping))
          geeky_file.close()

      except Exception as e:
          print(e)
          print("Unable to append to file")

  return df.drop("LengthofStay", axis = 1).values, df['LengthofStay'].values, df.drop("LengthofStay", axis=1).columns

# @title Training the model
df = pd.read_csv("/content/drive/MyDrive/data.csv")
r = int(X.shape[0] * 0.1)
X, Y, cols = transform_data(df)

train_X = X[:r*9,:]
train_Y = Y[:r*9]
test_X = X[r*9:,:]
test_Y = Y[r*9:]
rf = RandomForestRegressor(n_estimators=10, max_depth = 10)

rf.fit(train_X, train_Y)

rf.score(test_X, test_Y)

# @title Plotting the Shap Values
from shap import TreeExplainer
import shap
explainer = TreeExplainer(rf)
random_sample = test_X[np.random.randint(test_X.shape[0], size=50)]
shap_values = explainer.shap_values(random_sample)
shap.initjs()
shap.force_plot(explainer.expected_value, shap_values[0,:], random_sample[0,:], feature_names = cols)
shap.summary_plot(shap_values, random_sample, feature_names=cols)

# @title Rough code for Data Preprocessing
def clean_data(df):
    df = df.dropna(subset=['Health Service Area','Hospital County','Operating Certificate Number',
                      'Facility Id','Zip Code - 3 digits',
                      'APR Severity of Illness Description','APR Risk of Mortality'])
    df.drop(['CCS Diagnosis Description', 'CCS Procedure Description', 'APR DRG Description', 'APR MDC Description', 'Total Costs',
                  'APR Severity of Illness Description','Payment Typology 2','Payment Typology 3','Race','Gender','Birth Weight', 'Total Charges', 'Discharge Year','Abortion Edit Indicator',], axis=1, inplace=True)
    ga = math.pow(10,-15)
    df.loc[:, 'Length of Stay'] = df['Length of Stay'].replace('120 +',120)
    df.loc[:, 'Length of Stay'] = df['Length of Stay'].astype(np.int)

    df = df.sample(frac=1)

    names = df.cogelumns
    n = len(names)
    i = 0
    names1 = []
    def remove(string):
      return string.replace(" ", "")
    while(i < n):
        s = remove(names[i])
        if(s == "ZipCode-3digits"):
            s = "ZipCodedigits"
        names1.append(s)
        i = i+1

    df.columns = names1

    return df
def get_partial_data(columns):


    for col in columns:
        try:
          uniq_values = df[col].unique()
        except:
          print("the column giving error is, "+ col)
          break
        mapping = {k: v for v, k in enumerate(uniq_values)}
        df[col] = df[col].map(mapping)
        try:
            geeky_file = open('encodings.txt', 'a+')
            geeky_file.write("\n\n"+col+"\n"+str(mapping))
            geeky_file.close()

        except Exception as e:
            print(e)
            print("Unable to append to file")

    return df[columns].values, df['LengthofStay'].values

"""# Column wise"""

# @title Import
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

from torch.utils.data import Dataset,DataLoader, random_split
import math
from sklearn.ensemble import RandomForestRegressor

# @title Data Preprocessing and Transforming
def clean_data(df):
    df = df.dropna(subset=['Health Service Area','Hospital County','Operating Certificate Number',
                      'Facility Id','Zip Code - 3 digits',
                      'APR Severity of Illness Description','APR Risk of Mortality'])
    df.drop(['CCS Diagnosis Description', 'CCS Procedure Description', 'APR DRG Description', 'APR MDC Description', 'Total Costs', 'Total Charges', 'Discharge Year','Abortion Edit Indicator',
                  'APR Severity of Illness Description','Payment Typology 2','Payment Typology 3','Race','Gender','Birth Weight'], axis=1, inplace=True)
    ga = math.pow(10,-15)
    df.loc[:, 'Length of Stay'] = df['Length of Stay'].replace('120 +',120)
    df.loc[:, 'Length of Stay'] = df['Length of Stay'].astype(np.int)

    df = df.sample(frac=1)

    names = df.columns
    n = len(names)
    i = 0
    names1 = []
    def remove(string):
      return string.replace(" ", "")
    while(i < n):
        s = remove(names[i])
        if(s == "ZipCode-3digits"):
            s = "ZipCodedigits"
        names1.append(s)
        i = i+1

    df.columns = names1

    return df
def get_partial_data(df, columns):


    for col in columns:
        try:
          uniq_values = df[col].unique()
        except:
          print("the column giving error is, "+ col)
          break
        mapping = {k: v for v, k in enumerate(uniq_values)}
        df[col] = df[col].map(mapping)
        try:
            geeky_file = open('encodings.txt', 'a+')
            geeky_file.write("\n\n"+col+"\n"+str(mapping))
            geeky_file.close()

        except Exception as e:
            print(e)
            print("Unable to append to file")

    return df[columns].values, df['LengthofStay'].values
df = pd.read_csv('/content/drive/MyDrive/data.csv')
df = clean_data(df)

# @title Train and Evaluate using randomforest regressor
def train_and_evaluate(X, Y):
  r = int(X.shape[0] * 0.1)
  train_X = X[:r*9,:]
  train_Y = Y[:r*9]
  test_X = X[r*9:,:]
  test_Y = Y[r*9:]
  rf = RandomForestRegressor(n_estimators=10)

  rf.fit(train_X, train_Y)
  return rf.score(test_X, test_Y)

# @title Taking different columns
print("When I take columns, APR Severity of Illness Code")
X, Y = get_partial_data(df, ['APRSeverityofIllnessCode'])
print(f"The r2 score is {train_and_evaluate(X, Y)}")
print("When I take columns, APR DRG Code")
X, Y = get_partial_data(df, ['APRDRGCode'])
print(f"The r2 score is {train_and_evaluate(X, Y)}")
print("When I take columns, APR Severity of Illness Code, APR DRG Code")
X, Y = get_partial_data(df, ['APRSeverityofIllnessCode', 'APRDRGCode'])
print(f"The r2 score is {train_and_evaluate(X, Y)}")
print("When I take columns, APR MDC Code, APR DRG Code")
X, Y = get_partial_data(df, ['APRMDCCode', 'APRDRGCode'])
print(f"The r2 score is {train_and_evaluate(X, Y)}")
print("When I take columns, APR MDC Code, APR Severity of Illness Code")
X, Y = get_partial_data(df, ['APRMDCCode', 'APRSeverityofIllnessCode'])
print(f"The r2 score is {train_and_evaluate(X, Y)}")
print("When I take columns, APR MDC Code, APR Severity of Illness Code, APR DRG Code")
X, Y = get_partial_data(df, ['APRMDCCode', 'APRSeverityofIllnessCode', 'APRDRGCode'])
print(f"The r2 score is {train_and_evaluate(X, Y)}")
print("When I take columns, APR MDC Code, APR Severity of Illness Code, APR DRG Code, Patient Disposition")
X, Y = get_partial_data(df, ['APRMDCCode', 'APRSeverityofIllnessCode', 'APRDRGCode', 'PatientDisposition'])
print(f"The r2 score is {train_and_evaluate(X, Y)}")
print("When I take columns, APR MDC Code, APR Severity of Illness Code, APR DRG Code, Patient Disposition, CCS Procedure Code")
X, Y = get_partial_data(df, ['APRMDCCode', 'APRSeverityofIllnessCode', 'APRDRGCode', 'PatientDisposition', 'CCSProcedureCode'])
print(f"The r2 score is {train_and_evaluate(X, Y)}")
print("When I take columns, APR MDC Code, APR Severity of Illness Code, APR DRG Code, Patient Disposition, CCS Procedure Code, CCS Diagnosis Code")
X, Y = get_partial_data(df, ['APRMDCCode', 'APRSeverityofIllnessCode', 'APRDRGCode', 'PatientDisposition', 'CCSProcedureCode', 'CCSDiagnosisCode'])
print(f"The r2 score is {train_and_evaluate(X, Y)}")
print("When I take columns, APR MDC Code, APR Severity of Illness Code, APR DRG Code, Patient Disposition, CCS Procedure Code, CCS Diagnosis Code, Type of Admission")
X, Y = get_partial_data(df, ['APRMDCCode', 'APRSeverityofIllnessCode', 'APRDRGCode', 'PatientDisposition', 'CCSProcedureCode', 'CCSDiagnosisCode', 'TypeofAdmission'])
print(f"The r2 score is {train_and_evaluate(X, Y)}")
print("When I take columns, APR MDC Code, APR Severity of Illness Code, APR DRG Code, Patient Disposition, Type of Admission")
X, Y = get_partial_data(df, ['APRMDCCode', 'APRSeverityofIllnessCode', 'APRDRGCode', 'PatientDisposition', 'TypeofAdmission'])
print(f"The r2 score is {train_and_evaluate(X, Y)}")
print("When I take columns, APR MDC Code, APR Severity of Illness Code, APR DRG Code, Patient Disposition, APR Medical Surgical Description")
X, Y = get_partial_data(df, ['APRMDCCode', 'APRSeverityofIllnessCode', 'APRDRGCode', 'PatientDisposition', 'APRMedicalSurgicalDescription'])
print(f"The r2 score is {train_and_evaluate(X, Y)}")
print("When I take columns, APR MDC Code, APR Severity of Illness Code, APR DRG Code, Patient Disposition, APR Medical Surgical Description, Type of Admission")
X, Y = get_partial_data(df, ['APRMDCCode', 'APRSeverityofIllnessCode', 'APRDRGCode', 'PatientDisposition', 'APRMedicalSurgicalDescription', 'TypeofAdmission'])
print(f"The r2 score is {train_and_evaluate(X, Y)}")
print("When I take columns, APR MDC Code, APR Severity of Illness Code, APR DRG Code, Patient Disposition, Facility Name")
X, Y = get_partial_data(df, ['APRMDCCode', 'APRSeverityofIllnessCode', 'APRDRGCode', 'PatientDisposition',  'FacilityName'])
print(f"The r2 score is {train_and_evaluate(X, Y)}")

# @title Same as above code but using catBoostRegressor
!pip install catboost

from catboost import CatBoostRegressor

def get_partial_data(df, columns):


    for col in columns:
        try:
          uniq_values = df[col].unique()
        except:
          print("the column giving error is, "+ col)
          break
        mapping = {k: v for v, k in enumerate(uniq_values)}
        df[col] = df[col].map(mapping)
        try:
            geeky_file = open('encodings.txt', 'a+')
            geeky_file.write("\n\n"+col+"\n"+str(mapping))
            geeky_file.close()

        except Exception as e:
            print(e)
            print("Unable to append to file")

    return df[columns], df['LengthofStay'].values
def train_and_evaluate(X, Y, cat_features):
  r = int(X.shape[0] * 0.05)
  train_X = X.iloc[:r*18,:]
  train_Y = Y[:r*18]
  val_X = X.iloc[r*18:r*19,:]
  val_Y = Y[r*18:r*19]
  test_X = X.iloc[r*19:,:]
  test_Y = Y[r*19:]
  classifier = CatBoostRegressor(cat_features=cat_features, iterations = 300)

  classifier.fit(train_X, train_Y, eval_set = (val_X, val_Y))
  return classifier.score(test_X, test_Y)
print("When I take columns, APR Severity of Illness Code")
X, Y = get_partial_data(df, ['APRSeverityofIllnessCode'])
print(f"The r2 score is {train_and_evaluate(X, Y, ['APRSeverityofIllnessCode'])}")

print("When I take columns, APR DRG Code")
X, Y = get_partial_data(df, ['APRDRGCode'])
print(f"The r2 score is {train_and_evaluate(X, Y, ['APRDRGCode'])}")
print("When I take columns, APR Severity of Illness Code, APR DRG Code")
X, Y = get_partial_data(df, ['APRSeverityofIllnessCode', 'APRDRGCode'])
print(f"The r2 score is {train_and_evaluate(X, Y, ['APRSeverityofIllnessCode', 'APRDRGCode'])}")
print("When I take columns, APR MDC Code, APR DRG Code")
X, Y = get_partial_data(df, ['APRMDCCode', 'APRDRGCode'])
print(f"The r2 score is {train_and_evaluate(X, Y, ['APRMDCCode', 'APRDRGCode'])}")
print("When I take columns, APR MDC Code, APR Severity of Illness Code")
X, Y = get_partial_data(df, ['APRMDCCode', 'APRSeverityofIllnessCode'])
print(f"The r2 score is {train_and_evaluate(X, Y, ['APRMDCCode', 'APRSeverityofIllnessCode'])}")
print("When I take columns, APR MDC Code, APR Severity of Illness Code, APR DRG Code")
X, Y = get_partial_data(df, ['APRMDCCode', 'APRSeverityofIllnessCode', 'APRDRGCode'])
print(f"The r2 score is {train_and_evaluate(X, Y, ['APRMDCCode', 'APRSeverityofIllnessCode', 'APRDRGCode'])}")
print("When I take columns, APR MDC Code, APR Severity of Illness Code, APR DRG Code, Patient Disposition")
X, Y = get_partial_data(df, ['APRMDCCode', 'APRSeverityofIllnessCode', 'APRDRGCode', 'PatientDisposition'])
print(f"The r2 score is {train_and_evaluate(X, Y, ['APRMDCCode', 'APRSeverityofIllnessCode', 'APRDRGCode', 'PatientDisposition'])}")
print("When I take columns, APR MDC Code, APR Severity of Illness Code, APR DRG Code, Patient Disposition, CCS Procedure Code")
X, Y = get_partial_data(df, ['APRMDCCode', 'APRSeverityofIllnessCode', 'APRDRGCode', 'PatientDisposition', 'CCSProcedureCode'])
print(f"The r2 score is {train_and_evaluate(X, Y, ['APRMDCCode', 'APRSeverityofIllnessCode', 'APRDRGCode', 'PatientDisposition', 'CCSProcedureCode'])}")
print("When I take columns, APR MDC Code, APR Severity of Illness Code, APR DRG Code, Patient Disposition, CCS Procedure Code, CCS Diagnosis Code")
X, Y = get_partial_data(df, ['APRMDCCode', 'APRSeverityofIllnessCode', 'APRDRGCode', 'PatientDisposition', 'CCSProcedureCode', 'CCSDiagnosisCode'])
print(f"The r2 score is {train_and_evaluate(X, Y, ['APRMDCCode', 'APRSeverityofIllnessCode', 'APRDRGCode', 'PatientDisposition', 'CCSProcedureCode', 'CCSDiagnosisCode'])}")
print("When I take columns, APR MDC Code, APR Severity of Illness Code, APR DRG Code, Patient Disposition, CCS Procedure Code, CCS Diagnosis Code, Type of Admission")
X, Y = get_partial_data(df, ['APRMDCCode', 'APRSeverityofIllnessCode', 'APRDRGCode', 'PatientDisposition', 'CCSProcedureCode', 'CCSDiagnosisCode', 'TypeofAdmission'])
print(f"The r2 score is {train_and_evaluate(X, Y, ['APRMDCCode', 'APRSeverityofIllnessCode', 'APRDRGCode', 'PatientDisposition', 'CCSProcedureCode', 'CCSDiagnosisCode', 'TypeofAdmission'])}")
print("When I take columns, APR MDC Code, APR Severity of Illness Code, APR DRG Code, Patient Disposition, Type of Admission")
X, Y = get_partial_data(df, ['APRMDCCode', 'APRSeverityofIllnessCode', 'APRDRGCode', 'PatientDisposition', 'TypeofAdmission'])
print(f"The r2 score is {train_and_evaluate(X, Y, ['APRMDCCode', 'APRSeverityofIllnessCode', 'APRDRGCode', 'PatientDisposition', 'TypeofAdmission'])}")
print("When I take columns, APR MDC Code, APR Severity of Illness Code, APR DRG Code, Patient Disposition, APR Medical Surgical Description")
X, Y = get_partial_data(df, ['APRMDCCode', 'APRSeverityofIllnessCode', 'APRDRGCode', 'PatientDisposition', 'APRMedicalSurgicalDescription'])
print(f"The r2 score is {train_and_evaluate(X, Y, ['APRMDCCode', 'APRSeverityofIllnessCode', 'APRDRGCode', 'PatientDisposition', 'APRMedicalSurgicalDescription'])}")
print("When I take columns, APR MDC Code, APR Severity of Illness Code, APR DRG Code, Patient Disposition, APR Medical Surgical Description, Type of Admission")
X, Y = get_partial_data(df, ['APRMDCCode', 'APRSeverityofIllnessCode', 'APRDRGCode', 'PatientDisposition', 'APRMedicalSurgicalDescription', 'TypeofAdmission'])
print(f"The r2 score is {train_and_evaluate(X, Y, ['APRMDCCode', 'APRSeverityofIllnessCode', 'APRDRGCode', 'PatientDisposition', 'APRMedicalSurgicalDescription', 'TypeofAdmission'])}")
print("When I take columns, APR MDC Code, APR Severity of Illness Code, APR DRG Code, Patient Disposition, Facility Name")
X, Y = get_partial_data(df, ['APRMDCCode', 'APRSeverityofIllnessCode', 'APRDRGCode', 'PatientDisposition',  'FacilityName'])
print(f"The r2 score is {train_and_evaluate(X, Y, ['APRMDCCode', 'APRSeverityofIllnessCode', 'APRDRGCode', 'PatientDisposition',  'FacilityName'])}")

# @title Repeating the same using linear regressor
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim


import torch.optim.lr_scheduler as sch
from torch.utils.data import Dataset,DataLoader, random_split
import math
def transform_data(df, cols_to_oh_encode):
    index = 0
    arr = []
    total_oh_size = 0
    for col in cols_to_oh_encode:
        uniq_vals = df[col].unique()
        mapping = {k: v for v, k in enumerate(uniq_vals)}
        df[col] = df[col].map(mapping)
        arr.append(len(uniq_vals))
        total_oh_size += len(uniq_vals)
    return df[cols_to_oh_encode].values, df["LengthofStay"].values, arr, total_oh_size
class HospitalDataset(Dataset):
    def __init__(self, df, cols_to_oh_encode):
        self.X, self.Y, self.d, self.total_oh_size = transform_data(df, cols_to_oh_encode)
        self.cols_to_oh_encode = cols_to_oh_encode

    def __len__(self):
        return self.X.shape[0]

    def oh_encode(self, idx):
        x = self.X[idx]
        c = x.shape[0]
        X = np.zeros(self.total_oh_size)

        index = 0
        for i, j in enumerate(self.d):
            X[index + x[i]] = 1
            index += j


        return X
    def __getitem__(self, idx):
        Y = self.Y[idx]
        X = self.oh_encode(idx)

        return torch.tensor(X, dtype=torch.float32), torch.tensor(Y, dtype=torch.float32)
df = pd.read_csv('/content/drive/MyDrive/data.csv')
df = clean_data(df)
device = 'cuda' if torch.cuda.is_available() else 'cpu'
def train_and_evaluate(train_loader, val_loader, test_loader, no_of_features, epochs=4):
    model  = nn.Linear(no_of_features, 1).to(device)
    crit = nn.MSELoss()
    crit = crit.to(device)
    opti = optim.Adam(model.parameters(),lr=0.001,weight_decay=0.0001)
    lr = sch.StepLR(opti,step_size=5,gamma=0.1)


    i = 1

    model.eval()
    x_squared_sum = 0
    x_sum = 0
    o_x_sum = 0
    x_mean = 0
    o_squared_sum = 0
    for x, y in val_loader:

        with torch.no_grad():
            y = y.to(device)
            x = x.to(device)
            o = model(x)
            o = o.squeeze(-1)
            x_squared_sum += torch.sum(y**2)
            x_sum += torch.sum(y)
            o_x_sum += torch.sum (torch.dot(o, y))
            o_squared_sum += torch.sum(o**2)
            target_sum = torch.sum(y)
    x_mean = x_sum / val_size
    r2_score_numerator = x_squared_sum + o_squared_sum - 2 * o_x_sum
    r2_score_denominator = x_squared_sum + val_size * (x_mean**2) - 2 * (x_sum) * x_mean


    print(f"Before the start of epoch 1, r2 score = {1 - r2_score_numerator/r2_score_denominator}")

    while(i <= epochs):
        print(f"At epoch {i} of {epochs} epochs")
        model.train()
        epoch_loss = 0
        for X, Y in train_loader:
            X = X.to(device)
            Y = Y.to(device)


            output = model(X)

            loss = crit(torch.squeeze(output),Y.float())
            opti.zero_grad()
            loss.backward()
            epoch_loss += loss.item()
            opti.step()

        model.eval()
        x_squared_sum = 0
        x_sum = 0
        o_x_sum = 0
        x_mean = 0
        o_squared_sum = 0
        for x, y in val_loader:

          with torch.no_grad():
            y = y.to(device)
            x = x.to(device)
            o = model(x)
            o = o.squeeze(-1)
            x_squared_sum += torch.sum(y**2)
            x_sum += torch.sum(y)
            o_x_sum += torch.sum (torch.dot(o, y))
            o_squared_sum += torch.sum(o**2)
            target_sum = torch.sum(y)
          x_mean = x_sum / val_size
          r2_score_numerator = x_squared_sum + o_squared_sum - 2 * o_x_sum
          r2_score_denominator = x_squared_sum + val_size * (x_mean**2) - 2 * (x_sum) * x_mean


        print(f"At the end of epoch {i}, r2 score = {1 - r2_score_numerator/r2_score_denominator}, training loss = {epoch_loss}")

        i = i+1

    model.eval()
    x_squared_sum = 0
    x_sum = 0
    o_x_sum = 0
    x_mean = 0
    o_squared_sum = 0
    for x, y in test_loader:

      with torch.no_grad():
        y = y.to(device)
        x = x.to(device)
        o = model(x)
        o = o.squeeze(-1)
        x_squared_sum += torch.sum(y**2)
        x_sum += torch.sum(y)
        o_x_sum += torch.sum (torch.dot(o, y))
        o_squared_sum += torch.sum(o**2)
        target_sum = torch.sum(y)
      x_mean = x_sum / val_size
      r2_score_numerator = x_squared_sum + o_squared_sum - 2 * o_x_sum
      r2_score_denominator = x_squared_sum + val_size * (x_mean**2) - 2 * (x_sum) * x_mean


    print(f"Test r2 score = {1 - r2_score_numerator/r2_score_denominator}")

dataset = HospitalDataset(df, ['APRSeverityofIllnessCode'])
r = len(dataset)
train_size, val_size, test_size = int(r*0.9), int(r*0.05), r - int(r*0.05) - int(r*0.9)
train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42))
print(f"{train_size} training samples, {val_size} validation samples, {test_size} test samples")
train_loader = DataLoader(dataset=train_dataset, batch_size=70, shuffle=False)
val_loader = DataLoader(dataset=val_dataset, batch_size=70, shuffle=False)
test_loader = DataLoader(dataset=test_dataset, batch_size=70, shuffle=False)
train_and_evaluate(train_loader, val_loader, test_loader, dataset.total_oh_size)
def main(columns):
    dataset = HospitalDataset(df, columns)
    r = len(dataset)
    train_size, val_size, test_size = int(r*0.9), int(r*0.05), r - int(r*0.05) - int(r*0.9)
    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42))
    print(f"When columns {columns} are taken,")
    train_loader = DataLoader(dataset=train_dataset, batch_size=70, shuffle=False)
    val_loader = DataLoader(dataset=val_dataset, batch_size=70, shuffle=False)
    test_loader = DataLoader(dataset=test_dataset, batch_size=70, shuffle=False)
    train_and_evaluate(train_loader, val_loader, test_loader, dataset.total_oh_size)
column_arrays = [
                 ['APRSeverityofIllnessCode'],
                 ['APRDRGCode'],
                 ['APRSeverityofIllnessCode', 'APRDRGCode'],
                 ['APRMDCCode', 'APRDRGCode'],
                 ['APRMDCCode', 'APRSeverityofIllnessCode'],
                 ['APRMDCCode', 'APRSeverityofIllnessCode', 'APRDRGCode'],
                 ['APRMDCCode', 'APRSeverityofIllnessCode', 'APRDRGCode', 'PatientDisposition'],
                 ['APRMDCCode', 'APRSeverityofIllnessCode', 'APRDRGCode', 'PatientDisposition', 'CCSProcedureCode'],
                 ['APRMDCCode', 'APRSeverityofIllnessCode', 'APRDRGCode', 'PatientDisposition', 'CCSProcedureCode', 'CCSDiagnosisCode'],
                 ['APRMDCCode', 'APRSeverityofIllnessCode', 'APRDRGCode', 'PatientDisposition', 'CCSProcedureCode', 'CCSDiagnosisCode', 'TypeofAdmission'],
                 ['APRMDCCode', 'APRSeverityofIllnessCode', 'APRDRGCode', 'PatientDisposition', 'TypeofAdmission'],
                 ['APRMDCCode', 'APRSeverityofIllnessCode', 'APRDRGCode', 'PatientDisposition', 'APRMedicalSurgicalDescription'],
                 ['APRMDCCode', 'APRSeverityofIllnessCode', 'APRDRGCode', 'PatientDisposition', 'APRMedicalSurgicalDescription', 'TypeofAdmission'],
                 ['APRMDCCode', 'APRSeverityofIllnessCode', 'APRDRGCode', 'PatientDisposition',  'FacilityName'],
]
for column_array in column_arrays:
    main(column_array)

"""# Linear Regression again"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

from torch.utils.data import Dataset,DataLoader, random_split
import math
from sklearn.ensemble import RandomForestRegressor
def transform_data(df):
  df = df.dropna(subset=['Health Service Area','Hospital County','Operating Certificate Number',
                      'Facility Id','Zip Code - 3 digits',
                      'APR Severity of Illness Description','APR Risk of Mortality'])
  df.drop(['CCS Diagnosis Description', 'CCS Procedure Description', 'APR DRG Description', 'APR MDC Description', 'Total Costs',
                 'APR Severity of Illness Description','Payment Typology 2','Payment Typology 3','Race','Gender','Birth Weight'], axis=1, inplace=True)
  ga = math.pow(10,-15)
  df.loc[:, 'Length of Stay'] = df['Length of Stay'].replace('120 +',120)
  df.loc[:, 'Length of Stay'] = df['Length of Stay'].astype(np.int)

  df = df.sample(frac=1)

  names = df.columns
  n = len(names)
  i = 0
  names1 = []
  def remove(string):
    return string.replace(" ", "")
  while(i < n):
      s = remove(names[i])
      if(s == "ZipCode-3digits"):
          s = "ZipCodedigits"
      names1.append(s)
      i = i+1

  df.columns = names1

  obj_columns = df.select_dtypes(include=['object']).columns

  for col in obj_columns:
      try:
        uniq_values = df[col].unique()
      except:
        print("the column giving error is, "+ col)
        break
      mapping = {k: v for v, k in enumerate(uniq_values)}
      df[col] = df[col].map(mapping)
      try:
          geeky_file = open('encodings.txt', 'a+')
          geeky_file.write("\n\n"+col+"\n"+str(mapping))
          geeky_file.close()

      except Exception as e:
          print(e)
          print("Unable to append to file")

  return df.drop("LengthofStay", axis = 1).values, df['LengthofStay'].values
def transform_data(df):

  df.drop(['CCS Diagnosis Description', 'CCS Procedure Description', 'APR DRG Description', 'APR MDC Description', 'Total Costs',
                 'APR Severity of Illness Description','Payment Typology 2','Payment Typology 3','Race','Gender','Birth Weight'], axis=1, inplace=True)
  ga = math.pow(10,-15)
  df.loc[:, 'Length of Stay'] = df['Length of Stay'].replace('120 +',120)
  df.loc[:, 'Length of Stay'] = df['Length of Stay'].astype(np.int)
  x = np.where(df['Length of Stay'] >= 8,7,df['Length of Stay']-1)
  df.drop(['Length of Stay'], axis=1, inplace=True)
  df['Length of Stay'] = x
  df = df.sample(frac=1)

  names = df.columns
  n = len(names)
  i = 0
  names1 = []
  def remove(string):
    return string.replace(" ", "")
  while(i < n):
      s = remove(names[i])
      if(s == "ZipCode-3digits"):
          s = "ZipCodedigits"
      names1.append(s)
      i = i+1

  df.columns = names1
  r = df.shape[0]
  r = int(r/10)
  arr = []
  cols_to_oh_encode = ["HealthServiceArea", "HospitalCounty", "OperatingCertificateNumber", "FacilityId", "FacilityName", "AgeGroup", "ZipCodedigits", "Ethnicity", "TypeofAdmission", "PatientDisposition", "CCSDiagnosisCode", "CCSProcedureCode", "APRDRGCode", "APRMDCCode", "APRSeverityofIllnessCode", "APRRiskofMortality", "APRMedicalSurgicalDescription", "PaymentTypology1", "EmergencyDepartmentIndicator"]

  index = 0
  for col in cols_to_oh_encode:
    uniq_vals = df[col].unique()
    mapping = {k: v for v, k in enumerate(uniq_vals)}
    df[col] = df[col].map(mapping)
    arr.append(len(uniq_vals))
  return df[cols_to_oh_encode].values, df["LengthofStay"].values, arr, cols_to_oh_encode,
class HospitalDataset(Dataset):
    def __init__(self):
        df = pd.read_csv('/content/drive/MyDrive/data.csv')
        self.X, self.Y, self.d, self.cols_to_oh_encode = transform_data(df)


    def __len__(self):
        return self.X.shape[0]
    def oh_encode(self, idx):
        x = self.X[idx]

        c = x.shape[0]
        X1 = np.zeros(1610)
        X4 = np.ones(1)
        index = 0
        for i, j in enumerate(self.d):
            X1[index + x[i]] = 1
            index += j

        X = np.concatenate((X1,X4),axis=0)
        return X
    def __getitem__(self,idx):
        Y = self.Y[idx]
        X = self.oh_encode(idx)

        return torch.tensor(X, dtype=torch.float32), torch.tensor(Y, dtype=torch.float32)
dataset = HospitalDataset()
r = len(dataset)
train_size, val_size, test_size = int(r*0.9), int(r*0.05), r - int(r*0.05) - int(r*0.9)
train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42))
print(f"{train_size} training samples, {val_size} validation samples, {test_size} test samples")
train_loader = DataLoader(dataset=train_dataset, batch_size=70, shuffle=False)
val_loader = DataLoader(dataset=val_dataset, batch_size=70, shuffle=False)
test_loader = DataLoader(dataset=test_dataset, batch_size=70, shuffle=False)
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model  = nn.Linear(1611, 1).to(device)
crit = nn.MSELoss()
crit = crit.to(device)
opti = optim.Adam(model.parameters(),lr=0.001,weight_decay=0.0001)
lr = sch.StepLR(opti,step_size=5,gamma=0.1)
epochs = 10

i = 1
for x, y in val_loader:

    with torch.no_grad():
        y = y.to(device)
        x = x.to(device)
        o = model(x)
        o = o.squeeze(-1)
        x_squared_sum += torch.sum(y**2)
        x_sum += torch.sum(y)
        o_x_sum += torch.sum (torch.dot(o, y))
        o_squared_sum += torch.sum(o**2)
        target_sum = torch.sum(y)
x_mean = x_sum / val_size
r2_score_numerator = x_squared_sum + o_squared_sum - 2 * o_x_sum
r2_score_denominator = x_squared_sum + val_size * (x_mean**2) - 2 * (x_sum) * x_mean


print(f"Before the start of epoch 1, r2 score = {1 - r2_score_numerator/r2_score_denominator}")

while(i <= epochs):
    print(f"At epoch {i} of {epochs} epochs")
    model.train()
    epoch_loss = 0
    for X, Y in train_loader:
        X = X.to(device)
        Y = Y.to(device)


        output = model(X)

        loss = crit(torch.squeeze(output),Y.float())
        opti.zero_grad()
        loss.backward()
        epoch_loss += loss.item()
        opti.step()

    model.eval()
    x_squared_sum = 0
    x_sum = 0
    o_x_sum = 0
    x_mean = 0
    o_squared_sum = 0
    for x, y in val_loader:

      with torch.no_grad():
        y = y.to(device)
        x = x.to(device)
        o = model(x)
        o = o.squeeze(-1)
        x_squared_sum += torch.sum(y**2)
        x_sum += torch.sum(y)
        o_x_sum += torch.sum (torch.dot(o, y))
        o_squared_sum += torch.sum(o**2)
        target_sum = torch.sum(y)
      x_mean = x_sum / val_size
      r2_score_numerator = x_squared_sum + o_squared_sum - 2 * o_x_sum
      r2_score_denominator = x_squared_sum + val_size * (x_mean**2) - 2 * (x_sum) * x_mean


    print(f"At the end of epoch {i}, r2 score = {1 - r2_score_numerator/r2_score_denominator}, training loss = {epoch_loss}")

    i = i+1

model.eval()
x_squared_sum = 0
x_sum = 0
o_x_sum = 0
x_mean = 0
o_squared_sum = 0
for x, y in test_loader:

  with torch.no_grad():
    y = y.to(device)
    x = x.to(device)
    o = model(x)
    o = o.squeeze(-1)
    x_squared_sum += torch.sum(y**2)
    x_sum += torch.sum(y)
    o_x_sum += torch.sum (torch.dot(o, y))
    o_squared_sum += torch.sum(o**2)
    target_sum = torch.sum(y)
  x_mean = x_sum / val_size
  r2_score_numerator = x_squared_sum + o_squared_sum - 2 * o_x_sum
  r2_score_denominator = x_squared_sum + val_size * (x_mean**2) - 2 * (x_sum) * x_mean


print(f"Test r2 score = {1 - r2_score_numerator/r2_score_denominator}")

"""# CatBoost Regressor"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

from torch.utils.data import Dataset,DataLoader, random_split
import math

def clean_data(df):
    df.drop(df[df["Type of Admission"]=='Newborn'].index, axis = 0, inplace=True)
    df.drop(df[df["Type of Admission"]=='Unknown'].index, axis = 0, inplace=True)
    df.reset_index(inplace=True, drop=True)
    df = df.dropna(subset=['Health Service Area','Hospital County','Operating Certificate Number',
                      'Facility Id','Zip Code - 3 digits',
                      'APR Severity of Illness Description','APR Risk of Mortality'])
    df.drop(['CCS Diagnosis Description', 'CCS Procedure Description', 'APR DRG Description', 'APR MDC Description', 'Total Costs', 'Total Charges', 'Discharge Year','Abortion Edit Indicator',
                  'APR Severity of Illness Description','Payment Typology 2','Payment Typology 3','Race','Gender','Birth Weight'], axis=1, inplace=True)
    df[["Operating Certificate Number", "Facility Id"]] = df[["Operating Certificate Number", "Facility Id"]].astype(int)
    df.loc[:, 'Length of Stay'] = df['Length of Stay'].replace('120 +',120)
    df.loc[:, 'Length of Stay'] = df['Length of Stay'].astype(np.int)

    df = df.sample(frac=1)

    names = df.columns
    n = len(names)
    i = 0
    names1 = []
    def remove(string):
      return string.replace(" ", "")
    while(i < n):
        s = remove(names[i])
        if(s == "ZipCode-3digits"):
            s = "ZipCodedigits"
        names1.append(s)
        i = i+1

    df.columns = names1

    return df.drop("LengthofStay", axis = 1), df["LengthofStay"].values
from catboost import CatBoostRegressor
def train_and_evaluate(X, Y, cat_features):
  r = int(X.shape[0] * 0.05)
  train_X = X.iloc[:r*18,:]
  train_Y = Y[:r*18]
  val_X = X.iloc[r*18:r*19,:]
  val_Y = Y[r*18:r*19]
  test_X = X.iloc[r*19:,:]
  test_Y = Y[r*19:]
  classifier = CatBoostRegressor(cat_features=cat_features, iterations = 300)

  classifier.fit(train_X, train_Y, eval_set = (val_X, val_Y))
  return classifier, classifier.score(test_X, test_Y)
df = pd.read_csv('/content/drive/MyDrive/data.csv')

X, Y = clean_data(df)
clf, score = train_and_evaluate(X, Y, list(X.columns))
print(f"The r2 score is {score}")
!pip install shap
from shap import TreeExplainer
import shap
r = int(X.shape[0]/20)
test_X = X.iloc[r*19:,:]
test_Y = Y[r*19:]
test_X.shape[0]
explainer = shap.Explainer(clf)
random_sample = test_X.sample(10000, replace=True,random_state=42)
shap_values = explainer(random_sample)

shap_values.shape
columns = test_X.columns
map_columns_to_indices = {v: k for k, v in enumerate(columns)}
import matplotlib.pyplot as plt


shap.plots.scatter(shap_values[:, "APRSeverityofIllnessCode"])
shap.plots.beeswarm(shap_values, color="shap_red")

shap.summary_plot(shap_values, random_sample, feature_names=cols)

"""# Linear Regression with severity of illness code"""

# @title Import
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

import torch.optim.lr_scheduler as sch

from torch.utils.data import Dataset,DataLoader, random_split
import math

# @title Data preprocessing
def transform_data(df):

  df.drop(['CCS Diagnosis Description', 'CCS Procedure Description', 'APR DRG Description', 'APR MDC Description', 'Total Costs',
                 'APR Severity of Illness Description','Payment Typology 2','Payment Typology 3','Race','Gender','Birth Weight'], axis=1, inplace=True)
  ga = math.pow(10,-15)
  df.loc[:, 'Length of Stay'] = df['Length of Stay'].replace('120 +',120)
  df.loc[:, 'Length of Stay'] = df['Length of Stay'].astype(np.int)
  df = df.sample(frac=1)

  names = df.columns
  n = len(names)
  i = 0
  names1 = []
  def remove(string):
    return string.replace(" ", "")
  while(i < n):
      s = remove(names[i])
      if(s == "ZipCode-3digits"):
          s = "ZipCodedigits"
      names1.append(s)
      i = i+1

  df.columns = names1
  r = df.shape[0]
  r = int(r/10)
  arr = []
  cols_to_oh_encode = ["HealthServiceArea", "HospitalCounty", "OperatingCertificateNumber", "FacilityId", "FacilityName", "AgeGroup", "ZipCodedigits", "Ethnicity", "TypeofAdmission", "PatientDisposition", "CCSDiagnosisCode", "CCSProcedureCode", "APRDRGCode", "APRMDCCode", "APRRiskofMortality", "APRMedicalSurgicalDescription", "PaymentTypology1", "EmergencyDepartmentIndicator"]

  index = 0
  for col in cols_to_oh_encode:
    uniq_vals = df[col].unique()
    mapping = {k: v for v, k in enumerate(uniq_vals)}
    df[col] = df[col].map(mapping)
    arr.append(len(uniq_vals))
  return df[cols_to_oh_encode].values, df["LengthofStay"].values, arr, cols_to_oh_encode, df["APRSeverityofIllnessCode"].values

# @title Defining and running linear regression
class HospitalDataset(Dataset):
    def __init__(self):
        df = pd.read_csv('/content/drive/MyDrive/data.csv')
        self.X, self.Y, self.d, self.cols_to_oh_encode, self.severity = transform_data(df)


    def __len__(self):
        return self.X.shape[0]
    def oh_encode(self, idx):
        x = self.X[idx]
        severity = (self.severity[idx] + 1)
        c = x.shape[0]

        X1 = np.zeros(1610)
        X4 = np.ones(1)
        index = 0
        for i, j in enumerate(self.d):
            X1[index + x[i]] = severity
            index += j

        X = np.concatenate((X1,X4),axis=0)
        return X
    def __getitem__(self,idx):
        Y = self.Y[idx]
        X = self.oh_encode(idx)

        return torch.tensor(X, dtype=torch.float32), torch.tensor(Y, dtype=torch.float32)
dataset = HospitalDataset()
r = len(dataset)
train_size, val_size, test_size = int(r*0.9), int(r*0.15), r - int(r*0.05) - int(r*0.9)
train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42))
print(f"{train_size} training samples, {val_size} validation samples, {test_size} test samples")
train_loader = DataLoader(dataset=train_dataset, batch_size=70, shuffle=False)
val_loader = DataLoader(dataset=val_dataset, batch_size=70, shuffle=False)
test_loader = DataLoader(dataset=test_dataset, batch_size=70, shuffle=False)
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model  = nn.Linear(1611, 1).to(device)
crit = nn.MSELoss()
crit = crit.to(device)
opti = optim.Adam(model.parameters(),lr=0.001,weight_decay=0.0001)
lr = sch.StepLR(opti,step_size=5,gamma=0.1)
epochs = 10

i = 1
model.eval()
x_squared_sum = 0
x_sum = 0
o_x_sum = 0
x_mean = 0
o_squared_sum = 0

for x, y in val_loader:

    with torch.no_grad():
        y = y.to(device)
        x = x.to(device)
        o = model(x)
        o = o.squeeze(-1)
        x_squared_sum += torch.sum(y**2)
        x_sum += torch.sum(y)
        o_x_sum += torch.sum (torch.dot(o, y))
        o_squared_sum += torch.sum(o**2)
        target_sum = torch.sum(y)
x_mean = x_sum / val_size
r2_score_numerator = x_squared_sum + o_squared_sum - 2 * o_x_sum
r2_score_denominator = x_squared_sum + val_size * (x_mean**2) - 2 * (x_sum) * x_mean


print(f"Before the start of epoch 1, r2 score = {1 - r2_score_numerator/r2_score_denominator}")

while(i <= epochs):
    print(f"At epoch {i} of {epochs} epochs")
    model.train()
    epoch_loss = 0
    for X, Y in train_loader:
        X = X.to(device)
        Y = Y.to(device)


        output = model(X)

        loss = crit(torch.squeeze(output),Y.float())
        opti.zero_grad()
        loss.backward()
        epoch_loss += loss.item()
        opti.step()

    model.eval()
    x_squared_sum = 0
    x_sum = 0
    o_x_sum = 0
    x_mean = 0
    o_squared_sum = 0
    for x, y in val_loader:

      with torch.no_grad():
        y = y.to(device)
        x = x.to(device)
        o = model(x)
        o = o.squeeze(-1)
        x_squared_sum += torch.sum(y**2)
        x_sum += torch.sum(y)
        o_x_sum += torch.sum (torch.dot(o, y))
        o_squared_sum += torch.sum(o**2)
        target_sum = torch.sum(y)
      x_mean = x_sum / val_size
      r2_score_numerator = x_squared_sum + o_squared_sum - 2 * o_x_sum
      r2_score_denominator = x_squared_sum + val_size * (x_mean**2) - 2 * (x_sum) * x_mean


    print(f"At the end of epoch {i}, r2 score = {1 - r2_score_numerator/r2_score_denominator}, training loss = {epoch_loss}")

    i = i+1

model.eval()
x_squared_sum = 0
x_sum = 0
o_x_sum = 0
x_mean = 0
o_squared_sum = 0
for x, y in test_loader:

  with torch.no_grad():
    y = y.to(device)
    x = x.to(device)
    o = model(x)
    o = o.squeeze(-1)
    x_squared_sum += torch.sum(y**2)
    x_sum += torch.sum(y)
    o_x_sum += torch.sum (torch.dot(o, y))
    o_squared_sum += torch.sum(o**2)
    target_sum = torch.sum(y)
  x_mean = x_sum / val_size
  r2_score_numerator = x_squared_sum + o_squared_sum - 2 * o_x_sum
  r2_score_denominator = x_squared_sum + val_size * (x_mean**2) - 2 * (x_sum) * x_mean


print(f"Test r2 score = {1 - r2_score_numerator/r2_score_denominator}")
model  = nn.Linear(1611, 1).to(device)
crit = nn.MSELoss()
crit = crit.to(device)
opti = optim.Adam(model.parameters(),lr=0.001,weight_decay=0.0001)
lr = sch.StepLR(opti,step_size=5,gamma=0.1)
epochs = 10

i = 1
model.eval()
x_squared_sum = 0
x_sum = 0
o_x_sum = 0
x_mean = 0
o_squared_sum = 0

for x, y in val_loader:

    with torch.no_grad():
        y = y.to(device)
        x = x.to(device)
        o = model(x)
        o = o.squeeze(-1)
        x_squared_sum += torch.sum(y**2)
        x_sum += torch.sum(y)
        o_x_sum += torch.sum (torch.dot(o, y))
        o_squared_sum += torch.sum(o**2)
        target_sum = torch.sum(y)
x_mean = x_sum / val_size
r2_score_numerator = x_squared_sum + o_squared_sum - 2 * o_x_sum
r2_score_denominator = x_squared_sum + val_size * (x_mean**2) - 2 * (x_sum) * x_mean


print(f"Before the start of epoch 1, r2 score = {1 - r2_score_numerator/r2_score_denominator}")

while(i <= epochs):
    print(f"At epoch {i} of {epochs} epochs")
    model.train()
    epoch_loss = 0
    for X, Y in train_loader:
        X = X.to(device)
        Y = Y.to(device)


        output = model(X)

        loss = crit(torch.squeeze(output),Y.float())
        opti.zero_grad()
        loss.backward()
        epoch_loss += loss.item()
        opti.step()

    model.eval()
    x_squared_sum = 0
    x_sum = 0
    o_x_sum = 0
    x_mean = 0
    o_squared_sum = 0
    for x, y in val_loader:

      with torch.no_grad():
        y = y.to(device)
        x = x.to(device)
        o = model(x)
        o = o.squeeze(-1)
        x_squared_sum += torch.sum(y**2)
        x_sum += torch.sum(y)
        o_x_sum += torch.sum (torch.dot(o, y))
        o_squared_sum += torch.sum(o**2)
        target_sum = torch.sum(y)
      x_mean = x_sum / val_size
      r2_score_numerator = x_squared_sum + o_squared_sum - 2 * o_x_sum
      r2_score_denominator = x_squared_sum + val_size * (x_mean**2) - 2 * (x_sum) * x_mean


    print(f"At the end of epoch {i}, r2 score = {1 - r2_score_numerator/r2_score_denominator}, training loss = {epoch_loss}")

    i = i+1

model.eval()
x_squared_sum = 0
x_sum = 0
o_x_sum = 0
x_mean = 0
o_squared_sum = 0
for x, y in test_loader:

  with torch.no_grad():
    y = y.to(device)
    x = x.to(device)
    o = model(x)
    o = o.squeeze(-1)
    x_squared_sum += torch.sum(y**2)
    x_sum += torch.sum(y)
    o_x_sum += torch.sum (torch.dot(o, y))
    o_squared_sum += torch.sum(o**2)
    target_sum = torch.sum(y)
  x_mean = x_sum / val_size
  r2_score_numerator = x_squared_sum + o_squared_sum - 2 * o_x_sum
  r2_score_denominator = x_squared_sum + val_size * (x_mean**2) - 2 * (x_sum) * x_mean


print(f"Test r2 score = {1 - r2_score_numerator/r2_score_denominator}")




df = pd.read_csv('/content/drive/MyDrive/data.csv')
df["APR Severity of Illness Code"].unique()

# @title Plotting SHAP values
pip install shap
import shap


explainer = shap.DeepExplainer(model, next(iter(test_loader))[0].to(device),)
shap_values = explainer.shap_values(next(iter(test_loader))[0].to(device),)

str(type(shap_values))
d = dataset.d
def transform_numpy_matrix(m):
  index = 0
  x = np.zeros((m.shape[0], len(d)))
  for k, i in enumerate(d):
    x[:, k] = np.sum(m[:, index:index+i], axis=1)
    index += i
  return x
shap_values = [transform_numpy_matrix(m) for m in shap_values]

shap.summary_plot(shap_values,feature_names = dataset.cols_to_oh_encode, plot_type="bar")

shap.summary_plot(shap_values,feature_names = dataset.cols_to_oh_encode, plot_type="bar")

"""# Linear Regression removing type of admission 3


"""

# @title Importing libraries , performing necessary data preprocessing , Defining and runining Linear Regression
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.optim.lr_scheduler as sch
from torch.utils.data import Dataset,DataLoader, random_split
import math
def transform_data(df):

  df.drop(['CCS Diagnosis Description', 'CCS Procedure Description', 'APR DRG Description', 'APR MDC Description', 'Total Costs', 'Total Charges',
                 'APR Severity of Illness Description','Payment Typology 2','Payment Typology 3','Race','Gender','Birth Weight'], axis=1, inplace=True)
  print(df.shape[0])
  df.drop(df[df["Type of Admission"]=='Newborn'].index, axis = 0, inplace=True)
  df.drop(df[df["Type of Admission"]=='Unknown'].index, axis = 0, inplace=True)
  print(df.shape[0])
  df.reset_index(drop=True, inplace=True)
  ga = math.pow(10,-15)
  df.loc[:, 'Length of Stay'] = df['Length of Stay'].replace('120 +',120)
  df.loc[:, 'Length of Stay'] = df['Length of Stay'].astype(np.int)

  df = df.sample(frac=1)

  names = df.columns
  n = len(names)
  i = 0
  names1 = []
  def remove(string):
    return string.replace(" ", "")
  while(i < n):
      s = remove(names[i])
      if(s == "ZipCode-3digits"):
          s = "ZipCodedigits"
      names1.append(s)
      i = i+1

  df.columns = names1
  r = df.shape[0]
  r = int(r/10)
  arr = []
  cols_to_oh_encode = ["HealthServiceArea", "HospitalCounty", "OperatingCertificateNumber", "FacilityId", "FacilityName", "AgeGroup", "ZipCodedigits", "Ethnicity", "TypeofAdmission", "PatientDisposition", "CCSDiagnosisCode", "CCSProcedureCode", "APRDRGCode", "APRMDCCode", "APRRiskofMortality", "APRMedicalSurgicalDescription", "PaymentTypology1", "EmergencyDepartmentIndicator"]

  index = 0
  for col in cols_to_oh_encode:
    uniq_vals = df[col].unique()
    mapping = {k: v for v, k in enumerate(uniq_vals)}
    df[col] = df[col].map(mapping)
    arr.append(len(uniq_vals))
  return df[cols_to_oh_encode].values, df["LengthofStay"].values, arr, cols_to_oh_encode, df["APRSeverityofIllnessCode"].values
class HospitalDataset(Dataset):
    def __init__(self):
        df = pd.read_csv('/content/drive/MyDrive/data.csv')
        self.X, self.Y, self.d, self.cols_to_oh_encode, self.severity = transform_data(df)


    def __len__(self):
        return self.X.shape[0]
    def oh_encode(self, idx):
        x = self.X[idx]
        severity = (self.severity[idx] + 1)
        c = x.shape[0]

        X1 = np.zeros(1610)
        X4 = np.ones(1)
        index = 0
        for i, j in enumerate(self.d):
            X1[index + x[i]] = 1
            index += j

        X = np.concatenate((X1,X4),axis=0)
        return X
    def __getitem__(self,idx):
        Y = self.Y[idx]
        X = self.oh_encode(idx)

        return torch.tensor(X, dtype=torch.float32), torch.tensor(Y, dtype=torch.float32)
dataset = HospitalDataset()
r = len(dataset)
train_size, val_size, = int(r*0.9), r - int(r*0.9)
train_dataset, val_dataset = random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))
print(f"{train_size} training samples, {val_size} validation samples, {test_size} test samples")
train_loader = DataLoader(dataset=train_dataset, batch_size=70, shuffle=False)
val_loader = DataLoader(dataset=val_dataset, batch_size=70, shuffle=False)

device = 'cuda' if torch.cuda.is_available() else 'cpu'
model  = nn.Linear(1611, 1).to(device)
crit = nn.MSELoss()
crit = crit.to(device)
opti = optim.Adam(model.parameters(),lr=0.001,weight_decay=0.0001)
lr = sch.StepLR(opti,step_size=5,gamma=0.1)
epochs = 10

i = 1
model.eval()
x_squared_sum = 0
x_sum = 0
o_x_sum = 0
x_mean = 0
o_squared_sum = 0

for x, y in val_loader:

    with torch.no_grad():
        y = y.to(device)
        x = x.to(device)
        o = model(x)
        o = o.squeeze(-1)
        x_squared_sum += torch.sum(y**2)
        x_sum += torch.sum(y)
        o_x_sum += torch.sum (torch.dot(o, y))
        o_squared_sum += torch.sum(o**2)
        target_sum = torch.sum(y)
x_mean = x_sum / val_size
r2_score_numerator = x_squared_sum + o_squared_sum - 2 * o_x_sum
r2_score_denominator = x_squared_sum + val_size * (x_mean**2) - 2 * (x_sum) * x_mean


print(f"Before the start of epoch 1, r2 score = {1 - r2_score_numerator/r2_score_denominator}")

while(i <= epochs):
    print(f"At epoch {i} of {epochs} epochs")
    model.train()
    epoch_loss = 0
    for X, Y in train_loader:
        X = X.to(device)
        Y = Y.to(device)


        output = model(X)

        loss = crit(torch.squeeze(output),Y.float())
        opti.zero_grad()
        loss.backward()
        epoch_loss += loss.item()
        opti.step()

    model.eval()
    x_squared_sum = 0
    x_sum = 0
    o_x_sum = 0
    x_mean = 0
    o_squared_sum = 0
    for x, y in val_loader:

      with torch.no_grad():
        y = y.to(device)
        x = x.to(device)
        o = model(x)
        o = o.squeeze(-1)
        o = torch.round(o)
        x_squared_sum += torch.sum(y**2)
        x_sum += torch.sum(y)
        o_x_sum += torch.sum (torch.dot(o, y))
        o_squared_sum += torch.sum(o**2)
        target_sum = torch.sum(y)
      x_mean = x_sum / val_size
      r2_score_numerator = x_squared_sum + o_squared_sum - 2 * o_x_sum
      r2_score_denominator = x_squared_sum + val_size * (x_mean**2) - 2 * (x_sum) * x_mean


    print(f"At the end of epoch {i}, r2 score = {1 - r2_score_numerator/r2_score_denominator}, training loss = {epoch_loss}")

    i = i+1





model  = nn.Linear(1611, 1).to(device)
crit = nn.MSELoss()
crit = crit.to(device)
opti = optim.Adam(model.parameters(),lr=0.001,weight_decay=0.0001)
lr = sch.StepLR(opti,step_size=5,gamma=0.1)
epochs = 10

i = 1
model.eval()
x_squared_sum = 0
x_sum = 0
o_x_sum = 0
x_mean = 0
o_squared_sum = 0

for x, y in val_loader:

    with torch.no_grad():
        y = y.to(device)
        x = x.to(device)
        o = model(x)
        o = o.squeeze(-1)
        x_squared_sum += torch.sum(y**2)
        x_sum += torch.sum(y)
        o_x_sum += torch.sum (torch.dot(o, y))
        o_squared_sum += torch.sum(o**2)
        target_sum = torch.sum(y)
x_mean = x_sum / val_size
r2_score_numerator = x_squared_sum + o_squared_sum - 2 * o_x_sum
r2_score_denominator = x_squared_sum + val_size * (x_mean**2) - 2 * (x_sum) * x_mean


print(f"Before the start of epoch 1, r2 score = {1 - r2_score_numerator/r2_score_denominator}")

while(i <= epochs):
    print(f"At epoch {i} of {epochs} epochs")
    model.train()
    epoch_loss = 0
    for X, Y in train_loader:
        X = X.to(device)
        Y = Y.to(device)


        output = model(X)

        loss = crit(torch.squeeze(output),Y.float())
        opti.zero_grad()
        loss.backward()
        epoch_loss += loss.item()
        opti.step()

    model.eval()
    x_squared_sum = 0
    x_sum = 0
    o_x_sum = 0
    x_mean = 0
    o_squared_sum = 0
    for x, y in val_loader:

      with torch.no_grad():
        y = y.to(device)
        x = x.to(device)
        o = model(x)
        o = o.squeeze(-1)
        x_squared_sum += torch.sum(y**2)
        x_sum += torch.sum(y)
        o_x_sum += torch.sum (torch.dot(o, y))
        o_squared_sum += torch.sum(o**2)
        target_sum = torch.sum(y)
      x_mean = x_sum / val_size
      r2_score_numerator = x_squared_sum + o_squared_sum - 2 * o_x_sum
      r2_score_denominator = x_squared_sum + val_size * (x_mean**2) - 2 * (x_sum) * x_mean


    print(f"At the end of epoch {i}, r2 score = {1 - r2_score_numerator/r2_score_denominator}, training loss = {epoch_loss}")

    i = i+1

model.eval()
x_squared_sum = 0
x_sum = 0
o_x_sum = 0
x_mean = 0
o_squared_sum = 0
for x, y in test_loader:

  with torch.no_grad():
    y = y.to(device)
    x = x.to(device)
    o = model(x)
    o = o.squeeze(-1)
    x_squared_sum += torch.sum(y**2)
    x_sum += torch.sum(y)
    o_x_sum += torch.sum (torch.dot(o, y))
    o_squared_sum += torch.sum(o**2)
    target_sum = torch.sum(y)
  x_mean = x_sum / val_size
  r2_score_numerator = x_squared_sum + o_squared_sum - 2 * o_x_sum
  r2_score_denominator = x_squared_sum + val_size * (x_mean**2) - 2 * (x_sum) * x_mean


print(f"Test r2 score = {1 - r2_score_numerator/r2_score_denominator}")




df = pd.read_csv('/content/drive/MyDrive/data.csv')
df["APR Severity of Illness Code"].unique()

# @title Plotting shap values
pip install shap
import shap


explainer = shap.DeepExplainer(model, next(iter(test_loader))[0].to(device),)
shap_values = explainer.shap_values(next(iter(test_loader))[0].to(device),)

str(type(shap_values))
d = dataset.d
def transform_numpy_matrix(m):
  index = 0
  x = np.zeros((m.shape[0], len(d)))
  for k, i in enumerate(d):
    x[:, k] = np.sum(m[:, index:index+i], axis=1)
    index += i
  return x
shap_values = [transform_numpy_matrix(m) for m in shap_values]

shap.summary_plot(shap_values,feature_names = dataset.cols_to_oh_encode, plot_type="bar")

shap.summary_plot(shap_values,feature_names = dataset.cols_to_oh_encode, plot_type="bar")

"""# Logistic Regression on roughly balanced dataset




"""

# @title Imports
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.optim.lr_scheduler as sch
from sklearn.model_selection import KFold
from torch.utils.data import Dataset,DataLoader, random_split
import math

# @title Data Preprocessing
def transform_data(df):

  df.drop(['CCS Diagnosis Description', 'CCS Procedure Description', 'APR DRG Description', 'APR MDC Description',
                 'APR Severity of Illness Description','Payment Typology 2','Payment Typology 3','Race','Gender','Birth Weight'], axis=1, inplace=True)
  ga = math.pow(10,-15)

  df.loc[:, 'Length of Stay'] = df['Length of Stay'].replace('120 +',120)
  df.loc[:, 'Length of Stay'] = df['Length of Stay'].astype(np.int)

  df.drop(df[df["Type of Admission"]=='Newborn'].index, axis = 0, inplace=True)
  df.drop(df[df["Type of Admission"]=='Unknown'].index, axis = 0, inplace=True)
  df = df.sample(frac=1)
  def categorise(x):
    if (x==1):
      return 0
    elif (x==2):
      return 1
    elif (x==3):
      return 2
    elif (x<=6):
      return 3
    else:
      return 4
  df['Length of Stay'] = df['Length of Stay'].apply(categorise)

  names = df.columns
  n = len(names)
  i = 0
  names1 = []
  def remove(string):
    return string.replace(" ", "")
  while(i < n):
      s = remove(names[i])
      if(s == "ZipCode-3digits"):
          s = "ZipCodedigits"
      names1.append(s)
      i = i+1

  df.columns = names1
  r = df.shape[0]
  r = int(r/10)
  arr = []
  cols_to_oh_encode = ["HealthServiceArea", "HospitalCounty", "OperatingCertificateNumber", "FacilityId", "AgeGroup",  "TypeofAdmission", "PatientDisposition", "CCSDiagnosisCode", "CCSProcedureCode", "APRDRGCode", "APRMDCCode", "APRSeverityofIllnessCode", "APRRiskofMortality", "APRMedicalSurgicalDescription", "PaymentTypology1", "EmergencyDepartmentIndicator"]

  index = 0
  for col in cols_to_oh_encode:
    uniq_vals = df[col].unique()
    mapping = {k: v for v, k in enumerate(uniq_vals)}
    df[col] = df[col].map(mapping)
    arr.append(len(uniq_vals))
  return df[cols_to_oh_encode].values, df["LengthofStay"].values, arr, cols_to_oh_encode,

# @title Defining and running the logistic regression on roughly balanced dataset
class HospitalDataset(Dataset):
    def __init__(self):
        df = pd.read_csv('/content/drive/MyDrive/data.csv')
        self.X, self.Y, self.d, self.cols_to_oh_encode = transform_data(df)


    def __len__(self):
        return self.X.shape[0]
    def oh_encode(self, idx):
        x = self.X[idx]

        c = x.shape[0]
        X = np.zeros(1610)

        index = 0
        for i, j in enumerate(self.d):
            X[index + x[i]] = 1
            index += j


        return X
    def __getitem__(self,idx):
        Y = self.Y[idx]
        X = self.oh_encode(idx)

        return torch.tensor(X, dtype=torch.float32), torch.tensor(Y)
class LR(nn.Module):
    def __init__(self):
        super(LR, self).__init__()
        self.fc1 = nn.Linear(1610,5)

    def forward(self, x):
        x = self.fc1(x)
        x = F.softmax(x, dim=1)
        return x




device = 'cuda' if torch.cuda.is_available() else 'cpu'
k_f = KFold(n_splits=10,)
def train_and_evaluate(train_loader, val_loader, epochs=2):
  model  = LR().to(device)
  crit = nn.CrossEntropyLoss()
  crit = crit.to(device)
  opti = optim.Adam(model.parameters(),lr=0.001,weight_decay=0.0001)
  lr = sch.StepLR(opti,step_size=5,gamma=0.1)


  i = 1
  accuracy = 0
  while(i <= epochs):
    print(f"At epoch {i} of {epochs} epochs")
    model.train()
    epoch_loss = 0
    for X, Y in train_loader:
        X = X.to(device)
        Y = Y.to(device)


        output = model(X)

        loss = crit(output,Y.long())
        opti.zero_grad()
        loss.backward()
        epoch_loss += loss.item()
        opti.step()


    ncorr = 0
    nsam = 0
    model.eval()

    with torch.no_grad():
        for X, Y in val_loader:
            X = X.to(device)
            Y = Y.to(device)
            output = model(X)
            _, pred = output.max(1)
            nsam = nsam + Y.size(0)
            ncorr = ncorr + pred.eq(Y).sum().item()

    print(f"At the end of epoch {i}, {ncorr} correct predictions, accuracy = {ncorr/nsam}, training loss = {epoch_loss}")

    accuracy = ncorr/nsam
    i += 1
  return accuracy







dataset = HospitalDataset()
r = len(dataset)
train_val_size, test_size = int(r*0.9), int(r*0.1)
train_val_dataset, test_dataset = random_split(dataset, [train_val_size, test_size], generator=torch.Generator().manual_seed(42))
print(f"{train_val_size} training and val samples, {test_size} test samples")
train_val_loader = DataLoader(dataset=train_val_dataset, batch_size=70, shuffle=False)
test_loader = DataLoader(dataset=test_dataset, batch_size=70, shuffle=False)
for fold, (train_ids, val_ids) in enumerate(k_f.split(train_val_dataset)):
    # Print
    print(f'FOLD {fold}')
    print('--------------------------------')

    # Sample elements randomly from a given list of ids, no replacement.

    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)
    val_subsampler = torch.utils.data.SubsetRandomSampler(val_ids)
    train_loader = DataLoader(
                      dataset,
                      batch_size=70, sampler=train_subsampler)
    val_loader = DataLoader(
                      dataset,
                      batch_size=70, sampler=val_subsampler)

    train_and_evaluate(train_loader, val_loader, epochs=2)





model  = LR().to(device)
crit = nn.CrossEntropyLoss()
crit = crit.to(device)
opti = optim.Adam(model.parameters(),lr=0.001,weight_decay=0.0001)
lr = sch.StepLR(opti,step_size=5,gamma=0.1)

epochs=3
i = 1
accuracy = 0

while(i <= epochs):
  print(f"At epoch {i} of {epochs} epochs")
  model.train()
  epoch_loss = 0
  for X, Y in train_val_loader:
      X = X.to(device)
      Y = Y.to(device)


      output = model(X)

      loss = crit(output,Y.long())
      opti.zero_grad()
      loss.backward()
      epoch_loss += loss.item()
      opti.step()
  i += 1



ncorr = 0
nsam = 0
model.eval()
y = []
o = []
with torch.no_grad():
    for X, Y in test_loader:
        X = X.to(device)
        Y = Y.to(device)
        output = model(X)
        _, pred = output.max(1)
        y += list(Y.cpu().detach().numpy())
        o += list(pred.cpu().detach().numpy())
        nsam = nsam + Y.size(0)
        ncorr = ncorr + pred.eq(Y).sum().item()



accuracy = ncorr/nsam
print(accuracy)
sum(y)
y = np.array(y)
o = np.array(o)
def confmatrix(cl,Ytest,Ypred):
  n = len(Ytest)
  df = pd.DataFrame()
  i = 0
  while(i < cl):
    x = np.zeros(cl)
    j = 0
    sp = 0
    while(j < cl):
      x[j] = np.sum(np.where((Ytest==i)&(Ypred==j),1,0))
      sp += x[j]
      j = j+1
    num = x[i]/sp

    s = "Class"+str(i)
    df[s] = x
    i = i+1
  return df

df = confmatrix(5,y,o)

df
import matplotlib.pyplot as plt
import seaborn as sns
c = df.values
classes = ["1","2","3","4-6", ">6"]
df_cm = pd.DataFrame(c, index = classes,
                  columns = classes)
plt.figure(figsize = (10,7))
sns.heatmap(df_cm, annot=True,cmap='Blues',fmt='g')

# @title Plotting Shap values
pip install shap
import shap


explainer = shap.DeepExplainer(model, next(iter(test_loader))[0].to(device),)
shap_values = explainer.shap_values(next(iter(test_loader))[0].to(device),)

str(type(shap_values))
d = dataset.d
def transform_numpy_matrix(m):
  index = 0
  x = np.zeros((m.shape[0], len(d)))
  for k, i in enumerate(d):
    x[:, k] = np.sum(m[:, index:index+i], axis=1)
    index += i
  return x
shap_values = [transform_numpy_matrix(m) for m in shap_values]

shap.summary_plot(shap_values,feature_names = dataset.cols_to_oh_encode, plot_type="bar")

shap.summary_plot(shap_values,feature_names = dataset.cols_to_oh_encode, plot_type="bar")

"""# Linear Regression taking only type of admission 3


"""

# @title Importing libraries , performing necessary data preprocessing , Defining and runining Linear Regression
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.optim.lr_scheduler as sch
from torch.utils.data import Dataset,DataLoader, random_split
import math
def transform_data(df):

  df.drop(['CCS Diagnosis Description', 'CCS Procedure Description', 'APR DRG Description', 'APR MDC Description', 'Total Costs', 'Total Charges',
                 'APR Severity of Illness Description','Payment Typology 2','Payment Typology 3','Race','Gender'], axis=1, inplace=True)
  print(df.shape[0])
  df = df[df['Type of Admission']=='Newborn']
  df = df[df["Birth Weight"] > 0]
  print(df.shape[0])
  df.reset_index(drop=True, inplace=True)
  ga = math.pow(10,-15)
  df.loc[:, 'Length of Stay'] = df['Length of Stay'].replace('120 +',120)
  df.loc[:, 'Length of Stay'] = df['Length of Stay'].astype(np.int)

  df = df.sample(frac=1)

  names = df.columns
  n = len(names)
  i = 0
  names1 = []
  def remove(string):
    return string.replace(" ", "")
  while(i < n):
      s = remove(names[i])
      if(s == "ZipCode-3digits"):
          s = "ZipCodedigits"
      names1.append(s)
      i = i+1

  df.columns = names1
  r = df.shape[0]
  r = int(r/10)
  arr = []
  cols_to_oh_encode = ["HealthServiceArea", "HospitalCounty", "OperatingCertificateNumber", "FacilityId", "FacilityName", "AgeGroup", "ZipCodedigits", "Ethnicity", "TypeofAdmission", "PatientDisposition", "CCSDiagnosisCode", "CCSProcedureCode", "APRDRGCode", "APRMDCCode", "APRRiskofMortality", "APRMedicalSurgicalDescription", "PaymentTypology1", "EmergencyDepartmentIndicator"]

  index = 0
  for col in cols_to_oh_encode:
    uniq_vals = df[col].unique()
    mapping = {k: v for v, k in enumerate(uniq_vals)}
    df[col] = df[col].map(mapping)
    arr.append(len(uniq_vals))
  return df[cols_to_oh_encode].values, df["LengthofStay"].values, arr,
class HospitalDataset(Dataset):
    def __init__(self):
        df = pd.read_csv('/content/drive/MyDrive/data.csv')
        self.X, self.Y, self.d,  = transform_data(df)


    def __len__(self):
        return self.X.shape[0]
    def oh_encode(self, idx):
        x = self.X[idx]
        severity = (self.severity[idx] + 1)
        c = x.shape[0]

        X1 = np.zeros(1610)
        X4 = np.ones(1)
        index = 0
        for i, j in enumerate(self.d):
            X1[index + x[i]] = 1
            index += j

        X = np.concatenate((X1,X4),axis=0)
        return X
    def __getitem__(self,idx):
        Y = self.Y[idx]
        X = self.X[idx]

        return torch.tensor(X, dtype=torch.float32), torch.tensor(Y, dtype=torch.float32)
dataset = HospitalDataset()
r = len(dataset)
train_size, val_size, = int(r*0.9), r - int(r*0.9)
train_dataset, val_dataset = random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))
print(f"{train_size} training samples, {val_size} validation samples, no test samples")
train_loader = DataLoader(dataset=train_dataset, batch_size=70, shuffle=False)
val_loader = DataLoader(dataset=val_dataset, batch_size=70, shuffle=False)

device = 'cuda' if torch.cuda.is_available() else 'cpu'

x_squared_sum = 0
x_sum = 0
o_x_sum = 0
x_mean = 0
o_squared_sum = 0

for x, y in val_loader:
    y = y.to(device)
    x = x.to(device)
    o = torch.tensor([2.]).repeat( x.shape[0])
    o = o.squeeze(-1)
    x_squared_sum += torch.sum(y**2)
    x_sum += torch.sum(y)
    o_x_sum += torch.sum (torch.dot(o, y))
    o_squared_sum += torch.sum(o**2)
    target_sum = torch.sum(y)
x_mean = x_sum / val_size
r2_score_numerator = x_squared_sum + o_squared_sum - 2 * o_x_sum
r2_score_denominator = x_squared_sum + val_size * (x_mean**2) - 2 * (x_sum) * x_mean
print(f'r squared score is {1 - (r2_score_numerator/r2_score_denominator)}')
model  = nn.Linear(1611, 1).to(device)
crit = nn.MSELoss()
crit = crit.to(device)
opti = optim.Adam(model.parameters(),lr=0.001,weight_decay=0.0001)
lr = sch.StepLR(opti,step_size=5,gamma=0.1)
epochs = 10

i = 1
model.eval()


print(f"Before the start of epoch 1, r2 score = {1 - r2_score_numerator/r2_score_denominator}")

while(i <= epochs):
    print(f"At epoch {i} of {epochs} epochs")
    model.train()
    epoch_loss = 0
    for X, Y in train_loader:
        X = X.to(device)
        Y = Y.to(device)


        output = model(X)

        loss = crit(torch.squeeze(output),Y.float())
        opti.zero_grad()
        loss.backward()
        epoch_loss += loss.item()
        opti.step()

    model.eval()
    x_squared_sum = 0
    x_sum = 0
    o_x_sum = 0
    x_mean = 0
    o_squared_sum = 0
    for x, y in val_loader:

      with torch.no_grad():
        y = y.to(device)
        x = x.to(device)
        o = model(x)
        o = o.squeeze(-1)
        o = torch.round(o)
        x_squared_sum += torch.sum(y**2)
        x_sum += torch.sum(y)
        o_x_sum += torch.sum (torch.dot(o, y))
        o_squared_sum += torch.sum(o**2)
        target_sum = torch.sum(y)
      x_mean = x_sum / val_size
      r2_score_numerator = x_squared_sum + o_squared_sum - 2 * o_x_sum
      r2_score_denominator = x_squared_sum + val_size * (x_mean**2) - 2 * (x_sum) * x_mean


    print(f"At the end of epoch {i}, r2 score = {1 - r2_score_numerator/r2_score_denominator}, training loss = {epoch_loss}")

    i = i+1





model  = nn.Linear(1611, 1).to(device)
crit = nn.MSELoss()
crit = crit.to(device)
opti = optim.Adam(model.parameters(),lr=0.001,weight_decay=0.0001)
lr = sch.StepLR(opti,step_size=5,gamma=0.1)
epochs = 10

i = 1
model.eval()
x_squared_sum = 0
x_sum = 0
o_x_sum = 0
x_mean = 0
o_squared_sum = 0

for x, y in val_loader:

    with torch.no_grad():
        y = y.to(device)
        x = x.to(device)
        o = model(x)
        o = o.squeeze(-1)
        x_squared_sum += torch.sum(y**2)
        x_sum += torch.sum(y)
        o_x_sum += torch.sum (torch.dot(o, y))
        o_squared_sum += torch.sum(o**2)
        target_sum = torch.sum(y)
x_mean = x_sum / val_size
r2_score_numerator = x_squared_sum + o_squared_sum - 2 * o_x_sum
r2_score_denominator = x_squared_sum + val_size * (x_mean**2) - 2 * (x_sum) * x_mean


print(f"Before the start of epoch 1, r2 score = {1 - r2_score_numerator/r2_score_denominator}")

while(i <= epochs):
    print(f"At epoch {i} of {epochs} epochs")
    model.train()
    epoch_loss = 0
    for X, Y in train_loader:
        X = X.to(device)
        Y = Y.to(device)


        output = model(X)

        loss = crit(torch.squeeze(output),Y.float())
        opti.zero_grad()
        loss.backward()
        epoch_loss += loss.item()
        opti.step()

    model.eval()
    x_squared_sum = 0
    x_sum = 0
    o_x_sum = 0
    x_mean = 0
    o_squared_sum = 0
    for x, y in val_loader:

      with torch.no_grad():
        y = y.to(device)
        x = x.to(device)
        o = model(x)
        o = o.squeeze(-1)
        x_squared_sum += torch.sum(y**2)
        x_sum += torch.sum(y)
        o_x_sum += torch.sum (torch.dot(o, y))
        o_squared_sum += torch.sum(o**2)
        target_sum = torch.sum(y)
      x_mean = x_sum / val_size
      r2_score_numerator = x_squared_sum + o_squared_sum - 2 * o_x_sum
      r2_score_denominator = x_squared_sum + val_size * (x_mean**2) - 2 * (x_sum) * x_mean


    print(f"At the end of epoch {i}, r2 score = {1 - r2_score_numerator/r2_score_denominator}, training loss = {epoch_loss}")

    i = i+1

model.eval()
x_squared_sum = 0
x_sum = 0
o_x_sum = 0
x_mean = 0
o_squared_sum = 0
for x, y in test_loader:

  with torch.no_grad():
    y = y.to(device)
    x = x.to(device)
    o = model(x)
    o = o.squeeze(-1)
    x_squared_sum += torch.sum(y**2)
    x_sum += torch.sum(y)
    o_x_sum += torch.sum (torch.dot(o, y))
    o_squared_sum += torch.sum(o**2)
    target_sum = torch.sum(y)
  x_mean = x_sum / val_size
  r2_score_numerator = x_squared_sum + o_squared_sum - 2 * o_x_sum
  r2_score_denominator = x_squared_sum + val_size * (x_mean**2) - 2 * (x_sum) * x_mean


print(f"Test r2 score = {1 - r2_score_numerator/r2_score_denominator}")




df = pd.read_csv('/content/drive/MyDrive/data.csv')
df["APR Severity of Illness Code"].unique()

# @title Plotting Shap value
pip install shap
import shap


explainer = shap.DeepExplainer(model, next(iter(test_loader))[0].to(device),)
shap_values = explainer.shap_values(next(iter(test_loader))[0].to(device),)

str(type(shap_values))
d = dataset.d
def transform_numpy_matrix(m):
  index = 0
  x = np.zeros((m.shape[0], len(d)))
  for k, i in enumerate(d):
    x[:, k] = np.sum(m[:, index:index+i], axis=1)
    index += i
  return x
shap_values = [transform_numpy_matrix(m) for m in shap_values]

shap.summary_plot(shap_values,feature_names = dataset.cols_to_oh_encode, plot_type="bar")

shap.summary_plot(shap_values,feature_names = dataset.cols_to_oh_encode, plot_type="bar")

"""# The Decision Tree for non newborns"""

# @title Imports
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.optim.lr_scheduler as sch
from sklearn.model_selection import KFold
from torch.utils.data import Dataset,DataLoader, random_split
import math
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score

regressor = DecisionTreeRegressor(random_state=0)

# @title Using decision tree regressor
df = pd.read_csv('/content/drive/MyDrive/data.csv')
df = df[df["Type of Admission"]!='Newborn']
df = df[df["Type of Admission"]!='Unknown']
df.reset_index(inplace = True, drop = True)
df = df.sample(frac = 1)
r = int(df.shape[0]*0.1)
train_val_df = df.iloc[:r*9, :]
test_df = df.iloc[r*9:, :]

train_val_x, train_val_y, columns, target_encoding_dict = transform_data(train_val_df, train=True, target_encode = True)
test_x, test_y, columns, _ = transform_data(test_df, train=False, target_encode=True, target_encode_dict=target_encoding_dict)
train_val_x
cross_val_score(regressor, train_val_x, train_val_y, cv=10)

regressor.fit(train_val_x, train_val_y, check_input=True)
regressor.score(test_x, test_y)
!pip install sklearn-porter
from sklearn_porter import Porter
porter = Porter(clf, language='js')
output = porter.export(embed_data=True)
from sklearn.tree import export_graphviz
export_graphviz(regressor,out_file='tree_final.dot',
                feature_names = columns,
                rounded = True, proportion = False,
                precision = 2, filled = True)
regressor = DecisionTreeRegressor(random_state=0, max_depth=5, max_features="auto")
regressor.fit(train_val_x, train_val_y, check_input=True)
regressor.score(test_x, test_y)
feature_importances = regressor.feature_importances_

# @title Plots using pyplot and graphviz
import matplotlib.pyplot as plt
import numpy as np


plt.rcdefaults()
fig, ax = plt.subplots()

# Example data
features = columns
y_pos = np.arange(len(features))



ax.barh(y_pos, feature_importances, align='center')
ax.set_yticks(y_pos)
ax.set_yticklabels(columns)
ax.invert_yaxis()  # labels read top-to-bottom
ax.set_xlabel('Feature importance')
ax.set_title('Feature importance using a single Decision Tree')

plt.show()
from sklearn.tree import export_graphviz
dot_data = export_graphviz(regressor, out_file=None,
                feature_names = columns,
                rounded = True, proportion = False,
                precision = 2, filled = True)
import graphviz
graph = graphviz.Source(dot_data, format="png")
graph

"""# The Decision Tree for newborns without using Birth Weight




"""

# @title Imports
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.optim.lr_scheduler as sch
from sklearn.model_selection import KFold
from torch.utils.data import Dataset,DataLoader, random_split
import math
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score

regressor = DecisionTreeRegressor(random_state=0)

# @title Data Preprocessing
def transform_data(df, target_encode=False, train=True, target_encode_dict = dict()):
    df = df.dropna(subset=['Health Service Area','Hospital County','Operating Certificate Number',
                        'Facility Id','Zip Code - 3 digits',
                        'APR Severity of Illness Description','APR Risk of Mortality'])
    df.drop(['CCS Diagnosis Description', 'CCS Procedure Description','Discharge Year','Abortion Edit Indicator', 'APR DRG Description', 'APR MDC Description', 'Total Costs', 'Total Charges',
                  'APR Severity of Illness Description','Payment Typology 2','Payment Typology 3','Race','Gender','Birth Weight'], axis=1, inplace=True)


    df.loc[:, 'Length of Stay'] = df['Length of Stay'].replace('120 +',120)
    df.loc[:, 'Length of Stay'] = df['Length of Stay'].astype(np.int)



    names = df.columns
    n = len(names)
    i = 0
    names1 = []
    def remove(string):
      return string.replace(" ", "")
    while(i < n):
        s = remove(names[i])
        if(s == "ZipCode-3digits"):
            s = "ZipCodedigits"
        names1.append(s)
        i = i+1

    df.columns = names1
    output_target_encode_dict = {}
    if (target_encode and train):
        obj_columns = df.select_dtypes(include=['object']).columns
        for col in obj_columns:
            output_target_encode_dict[col]={}
            try:
                uniq_values = df[col].unique()
            except:
                print("the column giving error is, "+ col)
                break
            for uniq_val in uniq_values:
                target_values = df[df[col]==uniq_val]["LengthofStay"].values
                target_encoded_val = np.mean(target_values)*np.median(target_values)
                output_target_encode_dict[col][uniq_val] = target_encoded_val
                df.loc[df[col]==uniq_val, col] = target_encoded_val
    if (not train):
        obj_columns = df.select_dtypes(include=['object']).columns
        for col in obj_columns:
            try:
                uniq_values = df[col].unique()
            except:
                print("the column giving error is, "+ col)
                break
            for uniq_val in uniq_values:
                df.loc[df[col]==uniq_val, col] = target_encode_dict[col].get(uniq_val, 0)
    return df.drop("LengthofStay", axis = 1).values, df['LengthofStay'].values, df.drop("LengthofStay", axis=1).columns, output_target_encode_dict

# @title Using decision tree regressor
df = pd.read_csv('/content/drive/MyDrive/data.csv')
df = df[df["Type of Admission"]=='Newborn']
df = df[df["Birth Weight"]>0]
df.reset_index(inplace = True, drop = True)
df = df.sample(frac = 1)
r = int(df.shape[0]*0.1)
train_val_df = df.iloc[:r*9, :]
test_df = df.iloc[r*9:, :]

train_val_x, train_val_y, columns, target_encoding_dict = transform_data(train_val_df, train=True, target_encode = True)
test_x, test_y, columns, _ = transform_data(test_df, train=False, target_encode=True, target_encode_dict=target_encoding_dict)
train_val_x
regressor = DecisionTreeRegressor(random_state=0, max_depth=5, max_features="auto")
regressor.fit(train_val_x, train_val_y, check_input=True)
regressor.score(test_x, test_y)
feature_importances = regressor.feature_importances_

# @title Plotting using pyplot and graphviz
import matplotlib.pyplot as plt
import numpy as np



plt.rcdefaults()
fig, ax = plt.subplots()

# Example data
features = columns
y_pos = np.arange(len(features))



ax.barh(y_pos, feature_importances, align='center')
ax.set_yticks(y_pos)
ax.set_yticklabels(columns)
ax.invert_yaxis()  # labels read top-to-bottom
ax.set_xlabel('Feature importance')
ax.set_title('Feature importance using a single Decision Tree')

plt.show()
from sklearn.tree import export_graphviz
dot_data = export_graphviz(regressor, out_file=None,
                feature_names = columns,
                rounded = True, proportion = False,
                precision = 2, filled = True)
import graphviz
graph = graphviz.Source(dot_data, format="png")
graph

"""# The Decision Tree for newborns using Birth Weight




"""

# @title Imports
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.optim.lr_scheduler as sch
from sklearn.model_selection import KFold
from torch.utils.data import Dataset,DataLoader, random_split
import math
import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score

regressor = DecisionTreeRegressor(random_state=0)

# @title Data preprocessing
def transform_data(df, target_encode=False, train=True, target_encode_dict = dict()):
    df = df.dropna(subset=['Health Service Area','Hospital County','Operating Certificate Number',
                        'Facility Id','Zip Code - 3 digits',
                        'APR Severity of Illness Description','APR Risk of Mortality'])
    df.drop(['CCS Diagnosis Description', 'CCS Procedure Description','Discharge Year','Abortion Edit Indicator', 'APR DRG Description', 'APR MDC Description', 'Total Costs', 'Total Charges',
                  'APR Severity of Illness Description','Payment Typology 2','Payment Typology 3','Race','Gender'], axis=1, inplace=True)


    df.loc[:, 'Length of Stay'] = df['Length of Stay'].replace('120 +',120)
    df.loc[:, 'Length of Stay'] = df['Length of Stay'].astype(np.int)



    names = df.columns
    n = len(names)
    i = 0
    names1 = []
    def remove(string):
      return string.replace(" ", "")
    while(i < n):
        s = remove(names[i])
        if(s == "ZipCode-3digits"):
            s = "ZipCodedigits"
        names1.append(s)
        i = i+1

    df.columns = names1
    output_target_encode_dict = {}
    if (target_encode and train):
        obj_columns = df.select_dtypes(include=['object']).columns
        for col in obj_columns:
            output_target_encode_dict[col]={}
            try:
                uniq_values = df[col].unique()
            except:
                print("the column giving error is, "+ col)
                break
            for uniq_val in uniq_values:
                target_values = df[df[col]==uniq_val]["LengthofStay"].values
                target_encoded_val = np.mean(target_values)*np.median(target_values)
                output_target_encode_dict[col][uniq_val] = target_encoded_val
                df.loc[df[col]==uniq_val, col] = target_encoded_val
    if (not train):
        obj_columns = df.select_dtypes(include=['object']).columns
        for col in obj_columns:
            try:
                uniq_values = df[col].unique()
            except:
                print("the column giving error is, "+ col)
                break
            for uniq_val in uniq_values:
                df.loc[df[col]==uniq_val, col] = target_encode_dict[col].get(uniq_val, 0)
    return df.drop("LengthofStay", axis = 1).values, df['LengthofStay'].values, df.drop("LengthofStay", axis=1).columns, output_target_encode_dict

# @title Using decision tree regressor
df = pd.read_csv('/content/drive/MyDrive/data.csv')
df = df[df["Type of Admission"]=='Newborn']
df = df[df["Birth Weight"]>0]
df.reset_index(inplace = True, drop = True)
df = df.sample(frac = 1)
r = int(df.shape[0]*0.1)
train_val_df = df.iloc[:r*9, :]
test_df = df.iloc[r*9:, :]

train_val_x, train_val_y, columns, target_encoding_dict = transform_data(train_val_df, train=True, target_encode = True)
test_x, test_y, columns, _ = transform_data(test_df, train=False, target_encode=True, target_encode_dict=target_encoding_dict)
regressor = DecisionTreeRegressor(random_state=0, max_depth=3, max_features="auto")
regressor.fit(train_val_x, train_val_y, check_input=True)
regressor.score(test_x, test_y)
feature_importances = regressor.feature_importances_

# @title Plotting using pyplot and graphviz
import matplotlib.pyplot as plt
import numpy as np


plt.rcdefaults()
fig, ax = plt.subplots()

# Example data
features = columns
y_pos = np.arange(len(features))



ax.barh(y_pos, feature_importances, align='center')
ax.set_yticks(y_pos)
ax.set_yticklabels(columns)
ax.invert_yaxis()  # labels read top-to-bottom
ax.set_xlabel('Feature importance')
ax.set_title('Feature importance using a single Decision Tree')

plt.show()
from sklearn.tree import export_graphviz
dot_data = export_graphviz(regressor, out_file=None,
                feature_names = columns,
                rounded = True, proportion = False,
                precision = 2, filled = True)
import graphviz
graph = graphviz.Source(dot_data, format="png")
graph.save("single decision tree.dot")

!dot -Tjpeg "single decision tree.dot" -o "single decision tree.jpeg"

!gvpr -i 'N [level < 2]' "single decision tree.dot" | dot -Tjpeg -o "single decision tree trimmed.jpeg"

"""# The Decision Tree for newborns using Birth Weight and putting more depth





"""

# @title Imports
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.optim.lr_scheduler as sch
from sklearn.model_selection import KFold
from torch.utils.data import Dataset,DataLoader, random_split
import math
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score

regressor = DecisionTreeRegressor(random_state=0)

# @title Data Preprocessing
def transform_data(df, target_encode=False, train=True, target_encode_dict = dict()):
    df = df.dropna(subset=['Health Service Area','Hospital County','Operating Certificate Number',
                        'Facility Id','Zip Code - 3 digits',
                        'APR Severity of Illness Description','APR Risk of Mortality'])
    df.drop(['CCS Diagnosis Description', 'CCS Procedure Description','Discharge Year','Abortion Edit Indicator', 'APR DRG Description', 'APR MDC Description', 'Total Costs', 'Total Charges',
                  'APR Severity of Illness Description','Payment Typology 2','Payment Typology 3','Race','Gender'], axis=1, inplace=True)


    df.loc[:, 'Length of Stay'] = df['Length of Stay'].replace('120 +',120)
    df.loc[:, 'Length of Stay'] = df['Length of Stay'].astype(np.int)



    names = df.columns
    n = len(names)
    i = 0
    names1 = []
    def remove(string):
      return string.replace(" ", "")
    while(i < n):
        s = remove(names[i])
        if(s == "ZipCode-3digits"):
            s = "ZipCodedigits"
        names1.append(s)
        i = i+1

    df.columns = names1
    output_target_encode_dict = {}
    if (target_encode and train):
        obj_columns = df.select_dtypes(include=['object']).columns
        for col in obj_columns:
            output_target_encode_dict[col]={}
            try:
                uniq_values = df[col].unique()
            except:
                print("the column giving error is, "+ col)
                break
            for uniq_val in uniq_values:
                target_values = df[df[col]==uniq_val]["LengthofStay"].values
                target_encoded_val = np.mean(target_values)*np.median(target_values)
                output_target_encode_dict[col][uniq_val] = target_encoded_val
                df.loc[df[col]==uniq_val, col] = target_encoded_val
    if (not train):
        obj_columns = df.select_dtypes(include=['object']).columns
        for col in obj_columns:
            try:
                uniq_values = df[col].unique()
            except:
                print("the column giving error is, "+ col)
                break
            for uniq_val in uniq_values:
                df.loc[df[col]==uniq_val, col] = target_encode_dict[col].get(uniq_val, 0)
    return df.drop("LengthofStay", axis = 1).values, df['LengthofStay'].values, df.drop("LengthofStay", axis=1).columns, output_target_encode_dict

# @title Using decision tree regressor
df = pd.read_csv('/content/drive/MyDrive/data.csv')
df = df[df["Type of Admission"]=='Newborn']
df = df[df["Birth Weight"]>0]
df.reset_index(inplace = True, drop = True)
df = df.sample(frac = 1)
r = int(df.shape[0]*0.1)
train_val_df = df.iloc[:r*9, :]
test_df = df.iloc[r*9:, :]

train_val_x, train_val_y, columns, target_encoding_dict = transform_data(train_val_df, train=True, target_encode = True)
test_x, test_y, columns, _ = transform_data(test_df, train=False, target_encode=True, target_encode_dict=target_encoding_dict)
regressor = DecisionTreeRegressor(random_state=0, max_depth=7, max_features="auto")
regressor.fit(train_val_x, train_val_y, check_input=True)
regressor.score(test_x, test_y)
feature_importances = regressor.feature_importances_

# @title Plotting Using pyplot and graphviz
import matplotlib.pyplot as plt
import numpy as np


plt.rcdefaults()
fig, ax = plt.subplots()

# Example data
features = columns
y_pos = np.arange(len(features))



ax.barh(y_pos, feature_importances, align='center')
ax.set_yticks(y_pos)
ax.set_yticklabels(columns)
ax.invert_yaxis()  # labels read top-to-bottom
ax.set_xlabel('Feature importance')
ax.set_title('Feature importance using a single Decision Tree')

plt.show()
from sklearn.tree import export_graphviz
dot_data = export_graphviz(regressor, out_file=None,
                feature_names = columns,
                rounded = True, proportion = False,
                precision = 2, filled = True)
import graphviz
graph = graphviz.Source(dot_data, format="png")

